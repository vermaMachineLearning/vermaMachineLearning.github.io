<!DOCTYPE html>
<html>
  <head>
    <!-- Saurabh Verma 2016 UMN <https://github.com/vermaMachineLearning> -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="utf-8">
    
    <meta name="description" content="Blog: Machine Learning Equations by Saurabh Verma">
    <title>Blog: Machine Learning Equations by Saurabh Verma</title>
    <link rel="canonical" href="http://localhost:4000/">
    <link href="/main.css" rel="stylesheet" type="text/css">
    <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,200italic,300italic,400italic,600italic,700italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Gentium+Basic:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css' rel='stylesheet' type='text/css'>
    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blog: Machine Learning Equations by Saurabh Verma" />
    
    
	
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-78615814-1', 'auto');
  ga('send', 'pageview');

</script>

  </head>
  <body>
    <header>
      <h1><a href="/">Blog: Machine Learning Equations by Saurabh Verma</a></h1>
      <nav>
        <ul>
          <li><a href="/">Home</a></li><li><a href="/about.html">About</a></li><li><a href="/archive.html">Archive</a></li>
        </ul>
      </nav>
    </header>
    
    
    
    <article>
      <header>
        <h2><a href="/tutorials/2016/06/03/dirichlet-distribution.html"> Blog: Dirichlet Distributions.</a></h2>
        <p><time datetime="2016-06-03T19:01:00-05:00">Jun 3, 2016</time> • Tutorials</p>
      </header>
      <div>
<p>Dirichlet Distribution is <em>a distribution over distributions</em>. More specifically, it is a distribution over <em>pmfs (probability mass functions)</em>. You can imagine, as if there is a bag of <script type="math/tex">L</script> dices, and each dice has a corresponding pmf (related to six possible outcomes). Now picking a dice is like sampling a particular pmf from a distribution. The probability of picking a dice, which results in a pmf, comes from the Dirichlet distribution <script type="math/tex">Dir(\boldsymbol\alpha)</script>.</p>

<p>Let <script type="math/tex">\mathbf{q}=[q_1,q_2,...,q_k]</script> be a pmf (or a point in simplex <script type="math/tex">\in</script> <script type="math/tex">\mathbf{R}^{k}</script>), where <script type="math/tex">\sum\limits_{j=1}^{k}q_j=1</script> and <script type="math/tex">\mathbf{q} \sim Dir(\boldsymbol\alpha)</script>. Here <script type="math/tex">\boldsymbol\alpha</script> is dirichlet parameter <script type="math/tex">\boldsymbol\alpha=[\alpha_1,\alpha_2,...,\alpha_k]</script> and <script type="math/tex">\alpha_0=\sum\limits_{j=1}^{k}\alpha_j</script>. Then, the probability density of <script type="math/tex">\mathbf{q}</script> is given by:</p>

<script type="math/tex; mode=display">f(q,\boldsymbol\alpha)=p(q|\boldsymbol\alpha)=\frac{\Gamma(\alpha_0)}{\prod\limits_{j=1}^{k}\Gamma(\alpha_j)}\prod\limits_{j=1}^{k}q_j^{\alpha_j-1}</script>

<p><strong>Graphical Model:</strong></p>

<p>Suppose <script type="math/tex">\{x_i\}</script> is a set of samples drawn from <script type="math/tex">i^{th}</script> pmf where <script type="math/tex">i\in[1,L]</script>. For eg. <script type="math/tex">\{x_i\}</script> could be a sequence of outputs of a dice <script type="math/tex">\{1,2,1,3,4,3,6,..\}</script>. Let <script type="math/tex">\mathbf{q}_1,\mathbf{q}_2,...,\mathbf{q}_L</script> are <script type="math/tex">L</script> pmfs. Then:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{split}
p(\{x_i\}|\boldsymbol\alpha)&=\int p(\{x_i\},\mathbf{q}_i|\boldsymbol\alpha)d\mathbf{q}_i\\
&=\int p(\{x_i\}|\mathbf{q}_i,\boldsymbol\alpha) p(\mathbf{q}_i|\boldsymbol\alpha)  d\mathbf{q}_i\\
&=\int p(\{x_i\}|\mathbf{q}_i) p(\mathbf{q}_i|\boldsymbol\alpha)  d\mathbf{q}_i\\
\end{split}
\end{equation} %]]></script>

<p>Let <script type="math/tex">n_{ij}</script> be the number of outcomes in <script type="math/tex">\{x_i\}</script> sequence of samples that is equal to <script type="math/tex">j^{th}</script> event where <script type="math/tex">j\in[1,k]</script>, and let <script type="math/tex">n_i = \sum\limits_{j=1}^{k} n_{ij}</script>.</p>

<script type="math/tex; mode=display">p(\{x_i\}|\mathbf{q}_i)=\frac{n_i!}{\prod\limits_{j=1}^{k}n_{ij}!} \prod\limits_{j=1}^{k} q_{ij}^{n_{ij}}</script>

<p>Therefore,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{split}
p(\{x_i\}|\boldsymbol\alpha)&=\int \frac{n_i!}{\prod\limits_{j=1}^{k}n_{ij}!} \prod\limits_{j=1}^{k} q_{ij}^{n_{ij}} \times \frac{\Gamma(\alpha_0)}{\prod\limits_{j=1}^{k}\Gamma(\alpha_j)}\prod\limits_{j=1}^{k}q_{ij}^{\alpha_j-1}  d\mathbf{q}_i\\
&=  \frac{ \Gamma(\alpha_0) n_i! }{\prod\limits_{j=1}^{k}   \Gamma(\alpha_j) n_{ij}!}   \int \prod\limits_{j=1}^{k} q_{ij}^{n_{ij}+\alpha_j-1} d\mathbf{q}_i\\
&=\frac{ \Gamma(\alpha_0) n_i! }{\Gamma (\sum\limits_{j=1}^{k} (n_{ij}+ \alpha_j))}  \prod\limits_{j=1}^{k} \frac{ \Gamma(n_{ij}+\alpha_j) }{\Gamma(\alpha_j) n_{ij}!}\\
\end{split}
\end{equation} %]]></script>


      </div>
      <footer>
        <p><a href="/tutorials/2016/06/03/dirichlet-distribution.html">read more &raquo;</a></p>
      </footer>
    </article>
    
    <article>
      <header>
        <h2><a href="/tutorials/2016/06/01/bayesians-sets.html"> Blog: Bayesian Sets.</a></h2>
        <p><time datetime="2016-06-01T19:01:00-05:00">Jun 1, 2016</time> • Tutorials</p>
      </header>
      <div>
<p><strong>Bayesian Sets Graphical Model:</strong></p>
<div style="text-align:center"><img src="/images/bayesian-sets.svg" style="width: 35%; height: 35%" /></div>

<p>Bayesian sets are simple graphical models used to expand a set. For instance, suppose you are given a set of few words (or items) <script type="math/tex">S=\{cat, dog, lion, ...\}</script> which we refer as “seeds” and we wish to expand this set to include all similiar words from the given text corpus. Then, we can employ Bayesian sets which rank each item based on its importance of belonging to seed set.</p>

<p>Let <script type="math/tex">D</script> be a data set of items, and <script type="math/tex">\mathbf{x} \in D</script> be an item from this set. Assume the user provides a query set <script type="math/tex">D_c</script> which is a small subset of <script type="math/tex">D</script>, bayesian sets rank each item   by <script type="math/tex">score(\mathbf{x})</script>. This probability ratio  can be interpreted as the ratio of the joint probability of observing <script type="math/tex">x</script> and <script type="math/tex">D_c</script> to the probability of independently observing <script type="math/tex">x</script> and <script type="math/tex">D_c</script>.</p>

<blockquote>
  <script type="math/tex; mode=display">score(\mathbf{x})=\frac{p(\mathbf{x}\vert D_c)}{p(\mathbf{x})}= \frac{p(\mathbf{x}, D_c)}{p(\mathbf{x})p(D_c)}=\frac{\int p(\mathbf{x},\theta \vert D_c ) d\theta}{\int p(\mathbf{x},\theta) d\theta}=\frac{\int p(\mathbf{x} \vert \theta, D_c ) p(\theta \vert D_c) d\theta}{\int p(\mathbf{x} \vert \theta) p(\theta )d\theta}</script>
</blockquote>

<p>Assume that the parameterized model is <script type="math/tex">p(\mathbf{x} \vert \theta)</script> where <script type="math/tex">\theta</script> are the parameters as shown in figure above. Here, we assume that <script type="math/tex">\mathbf{x}</script> is represented by a binary feature vector and <script type="math/tex">\theta_j</script> is the weight associated with feature <script type="math/tex">j</script>. Then,</p>

<p>For each <script type="math/tex">\mathbf{x}_i</script> (Note: <script type="math/tex">\mathbf{x}</script> vector <script type="math/tex">j^{th}</script> component is <script type="math/tex">x_{.j}</script> and bold letters are vectors):</p>

<script type="math/tex; mode=display">p(\mathbf{x}_i\vert \boldsymbol\theta)=\prod\limits_{j=1}^{J} \theta_{j}^{x_{ij}} (1-\theta_j)^{1-x_{ij}}</script>

<p>The conjugate prior for the parameters of a Bernoulli distribution is the Beta distribution:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{split}
p(\boldsymbol\theta\vert \boldsymbol\alpha, \boldsymbol\beta) &=\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}	 \theta_{j}^{\alpha_j-1} (1-\theta_j)^{\beta_j-1} \\
p(\mathbf{x} \vert \boldsymbol\alpha, \boldsymbol\beta) &=\int p(\mathbf{x} \vert \theta, \boldsymbol\alpha, \boldsymbol\beta) p(\theta )d\theta=\int p(\mathbf{x} \vert \theta) p(\theta \vert \boldsymbol\alpha, \boldsymbol\beta )d\theta \\
&=\int \prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}	 \theta_{j}^{\alpha_j-1+x_{.j}} (1-\theta_j)^{\beta_j -x_{.j} } d\theta\\
&= \prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}	 \int_{0}^{1}  \theta_{j}^{\alpha_j-1+x_{.j}} (1-\theta_j)^{\beta_j -x_{.j} } d\theta_j \\
&=\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)} \frac{\Gamma(\alpha_j+x_{.j})\Gamma(\beta_j+1-x_{.j})}{\Gamma(\alpha_j+\beta_j+1)}	\\
p(D_c \vert \boldsymbol\alpha, \boldsymbol\beta)&=\int p(D_c, \theta) d\theta=\int p(D_c \vert \theta)p(\theta \vert \boldsymbol\alpha, \boldsymbol\beta) d\theta \\
&=\int \Big(\prod\limits_{i=1}^{N} p(\mathbf{x}_i\vert \theta)\Big)p(\theta \vert \boldsymbol\alpha, \boldsymbol\beta) d\theta \\
&=\int \Big(\prod\limits_{i=1}^{N}\Big(\prod\limits_{j=1}^{J} \theta_{j}^{x_{ij}} (1-\theta_j)^{1-x_{ij}}\Big)\Big)\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}	 \theta_{j}^{\alpha_j-1} (1-\theta_j)^{\beta_j-1}d\theta \\
&=\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}	 \int_{0}^{1}\theta_{j}^{\alpha_j-1+\sum_{i=1}^{N}x_{ij}} (1-\theta_j)^{\beta_j -\sum_{i=1}^{N}x_{.j}+N+1 } d\theta_j \\
&= \prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}   \frac{\Gamma(\widetilde\alpha_j)\Gamma(\widetilde\beta_j)}{\Gamma(\widetilde\alpha_j+\widetilde\beta_j)} \\
&\text{where},  \widetilde\alpha_j= \alpha_j+\sum_{i=1}^{N}x_{ij} , \hspace{1em} \widetilde\beta_j=\beta_j+N-\sum_{i=1}^{N}x_{ij}\\
\end{split}
\end{equation} %]]></script>


      </div>
      <footer>
        <p><a href="/tutorials/2016/06/01/bayesians-sets.html">read more &raquo;</a></p>
      </footer>
    </article>
    
    <article>
      <header>
        <h2><a href="/paperlist/2016/05/31/why-autoencoder-sparse.html"> Blog: Why Regularized Auto-Encoders learn Sparse Representation?</a></h2>
        <p><time datetime="2016-05-31T17:37:00-05:00">May 31, 2016</time> • PaperList</p>
      </header>
      <div>
<p>This paper has some great insights to offer in design of autoencoders. As title suggests, it addresses “whether regularized helps in learning sparse representation of the data theoretically?”</p>

<p><strong>Here is the setup:</strong> we have an input <script type="math/tex">\mathbf{x} \in \mathbf{R}^{d}</script> which is mapped to latent space <script type="math/tex">\mathbf{h=s(Wx+b)}</script> via autoencoder where <script type="math/tex">\mathbf{s}</script> is encoder activation function, <script type="math/tex">\mathbf{W} \in \mathbf{R}^{m\times n}</script> is the weight matrix, and <script type="math/tex">\mathbf{b} \in \mathbf{R^m}</script> is the encoder bias and <script type="math/tex">\mathbf{h} \in \mathbf{R^m}</script> is hidden representation or outputs of hidden units.</p>

<p>For analysis, paper assumes that decoder is linear i.e. <script type="math/tex">\mathbf{W^{T}h}</script> decodes back the encoded hidden representation and loss is squared loss function. Therefore, for learning <script type="math/tex">\mathbf{W,b}</script> parameters of autoencoder; objective function is:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{split}
J_{AE} &=\mathbb{E_x}[(\mathbf{x-W^Th})^2] \\
&= \mathbb{E_x}[(\mathbf{x-W^Ts(Wx+b)})^2]\\
\end{split}
\end{equation} %]]></script>

<blockquote>
  <p>Now we are interested in sparsity of <script type="math/tex">\mathbf{h}</script>, hidden representation.</p>
</blockquote>

<p>Besides above, paper makes two more assumptions.</p>

<blockquote>
  <p><strong>Assumption 1</strong>: Data is drawn from a distribution <script type="math/tex">\mathbf{x} \sim  X</script> for which <script type="math/tex">\mathbb{E_x}[\mathbf{x}]=0</script> and <script type="math/tex">\mathbb{E_x}[\mathbf{xx^T]=I}</script>, where <script type="math/tex">\mathbf{I}</script> is identity matrix.</p>
</blockquote>

<p>Basically, it assumes that data is whitened which is reasonable for some cases. There is one more assumption from analysis point of view which is needed for the derivation of theorems, we will see that later.</p>

<p>Finally, for a give data sample <script type="math/tex">\mathbf{x}</script>, each hidden unit <script type="math/tex">h_j</script> gets activated if pre-activation unit <script type="math/tex">a_j</script> is greater than <script type="math/tex">\delta</script> threshold:</p>

<script type="math/tex; mode=display">a_j=W_j\mathbf{x}+b_j</script>

<script type="math/tex; mode=display">h_j=s(a_j)</script>


      </div>
      <footer>
        <p><a href="/paperlist/2016/05/31/why-autoencoder-sparse.html">read more &raquo;</a></p>
      </footer>
    </article>
    
    <article>
      <header>
        <h2><a href="/paperlist/2016/05/29/convgraph.html">Blog: Learning Convolutional Networks for Graphs</a></h2>
        <p><time datetime="2016-05-29T17:37:00-05:00">May 29, 2016</time> • PaperList</p>
      </header>
      <div>
<p>For past couple of months, I have been wondering about how convolutional networks can be applied to  graphs. As we know, convolutional networks have became the state of the art in image classification and also in natural language processing. It is easy to see how convolutional networks  exploits the locality and translation invariance properties in an image. People have now also realized on how to exploit those properties in NLP using convolutional networks efficiently. It is time to look at other, more general, domains such as  <em>graphs</em> where notion of locality or receptive field needs to be defined. At this point, we can start thinking about graph neighborhood concepts as it is going to play a major role in connecting with receptive fields of convolutional networks.</p>


      </div>
      <footer>
        <p><a href="/paperlist/2016/05/29/convgraph.html">read more &raquo;</a></p>
      </footer>
    </article>
    
    <article>
      <header>
        <h2><a href="/paperlist/2016/05/28/ReadingListICML16.html">Reading List: ICML Papers 2016 </a></h2>
        <p><time datetime="2016-05-28T17:37:00-05:00">May 28, 2016</time> • PaperList</p>
      </header>
      <div>
<p>To make a habit of reading more papers, I have decided to write comments, may be some quick or sometime detail, on  the papers which I find interesting and related to my research area. Hopefully, this will come handy in solving my own research problems.</p>

<p><strong>Starting with ICML 2016 Papers:</strong></p>

<ul>
  <li>Revisiting Semi-Supervised Learning with Graph Embeddings ICML 2016</li>
  <li>Why Regularized Auto-Encoders learn  Sparse Representation? ICML 2016</li>
  <li>On the Consistency of Feature Selection With Lasso for Non-linear Targets. ICML 2016</li>
  <li>Additive Approximations in High Dimensional Nonparametric Regression via the SALSA. ICML 2016</li>
  <li>The Variational Nystrom method for large-scale spectral problems. ICML 2016</li>
  <li>A Kernelized Stein Discrepancy for Goodness-of-fit Tests and Model Evaluation. ICML 2016 (Possibly a new way to measure difference in probability distribution other than most common – KL divergence )</li>
  <li>Low-Rank Matrix Approximation with Stability. ICML 2016</li>
  <li>Unsupervised Deep Embedding for Clustering Analysis. ICML 2016</li>
  <li>Online Low-Rank Subspace Clustering by Explicit Basis Modeling. ICML 2016</li>
  <li>Community Recovery in Graphs with Locality. ICML 2016</li>
  <li>Analysis of Deep Neural Networks with Extended Data Jacobian Matrix. ICML 2016</li>
  <li>Stochastic Variance Reduced Optimization for Nonconvex Sparse Learning. ICML 2016</li>
  <li>Compressive Spectral Clustering ICML. 2016</li>
  <li>Variance-Reduced and Projection-Free Stochastic Optimization. ICML 2016</li>
  <li>Learning Convolutional Neural Networks for Graphs. ICML 2016</li>
  <li>Discrete Deep Feature Extraction: A Theory and New Architectures.  ICML 2016</li>
</ul>

      </div>
      <footer>
        <p><a href="/paperlist/2016/05/28/ReadingListICML16.html">read more &raquo;</a></p>
      </footer>
    </article>
    
    
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <footer>
      <span><a href="http://localhost:4000">Saurabh Verma</a></span>
      <span><a href="https://github.com/vermaMachineLearning/"><i class="fa fa-github-square"></i></a><a href="https://www.linkedin.com/in/saurabh-verma-10076544"><i class="fa fa-linkedin-square"></i></a><a href="https://www.facebook.com/saurabh.verma.355"><i class="fa fa-facebook-square"></i></a></span>
      <span>&copy; 2017</span>
    </footer>
  </body>
</html>
