<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog: Machine Learning Equations by Saurabh Verma</title>
    <description>Blog: Machine Learning Equations by Saurabh Verma</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 29 Oct 2017 15:13:06 -0500</pubDate>
    <lastBuildDate>Sun, 29 Oct 2017 15:13:06 -0500</lastBuildDate>
    <generator>Jekyll v3.6.0</generator>
    
      <item>
        <title> Blog: Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs (NIPS 2017).</title>
        <description>&lt;div style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/images/motivation.png&quot; style=&quot;width: 50%; height: 50%&quot; /&gt;&lt;/div&gt;
&lt;div style=&quot;text-align:center&quot;&gt;How can we convert a &lt;span style=&quot;color:#D35400&quot;&gt;graph&lt;/span&gt; into a &lt;span style=&quot;color:#006C31&quot;&gt;Feature Vector&lt;/span&gt;?&lt;/div&gt;

&lt;p&gt;Graph is a fundamental but complicated structure to work with from machine learning point of view. Think about it, a machine learning model usually require inputs in some form of mathematical objects like vectors, matrices, real number sequences and then produces the desired outputs. So, the question is how we can convert a graph into a mathematical object that is suitable for performing machine learning tasks such as classification or regression on graphs.&lt;/p&gt;

&lt;p&gt;For past one year, I have been working on developing machine learning models for graph(s) in order to solve two specific problems:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1) &lt;strong&gt;Graph Classification&lt;/strong&gt;: Given a collection of graphs &lt;script type=&quot;math/tex&quot;&gt;\{G_i\}_{i=1}^{M}&lt;/script&gt; and associated graph labels &lt;script type=&quot;math/tex&quot;&gt;\{y_i\}_{i=1}^{M}&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;y_i\in \mathbf{R^{c}}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt; is the number of classes), we need to come up with a learning model or function &lt;script type=&quot;math/tex&quot;&gt;F:G \rightarrow Y&lt;/script&gt; that can classify or predict the label associated with a graph.  &lt;br /&gt;
&lt;br /&gt;
2) &lt;strong&gt;Node Classification&lt;/strong&gt;: Given a graph &lt;script type=&quot;math/tex&quot;&gt;G=(V,E,W)&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; is vertex set, &lt;script type=&quot;math/tex&quot;&gt;E&lt;/script&gt; is edge set &amp;amp; &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt; is adjacency matrix and labels &lt;script type=&quot;math/tex&quot;&gt;\{y_i\}_{i=1}^{M}&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;y_i\in \mathbf{R^{c}}&lt;/script&gt; associated with each node, our task is to classify or predict the labels on each node. That is, come up a learning function &lt;script type=&quot;math/tex&quot;&gt;F:V\rightarrow Y&lt;/script&gt;.&lt;/p&gt;
  &lt;div style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/images/graph_problems.png&quot; style=&quot;width: 50%; height: 50%&quot; /&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this blog, I’ll mainly focus on graph classification problem but also show how we can leverage this work to perform node label classification as well. The fundamental problem in graph classification is comparing two graph structures. The main hurdle in comparing two graph structures is that we don’t know which node in the first graph corresponds to which node in other graph. This problem is universally known as graph isomorphism problem and so far we know it can only be solved in quasi-polynomial time and that too theoretically. Hence people came up with approximate but polynomial alternatives.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Previous studies on graph classification can be grouped into three main categories. &lt;br /&gt;
1) &lt;strong&gt;Graph Spectrum&lt;/strong&gt; 2) &lt;strong&gt;Graph Kernels&lt;/strong&gt; 3) &lt;strong&gt;Graph Convolutional Networks&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Graph Spectrum&lt;/strong&gt;: Our majority work falls under first category and similar to graph kernels in certain aspects. But without going too much into &lt;script type=&quot;math/tex&quot;&gt;-&lt;/script&gt; what is graph kernel and graph convolutional network &lt;script type=&quot;math/tex&quot;&gt;-&lt;/script&gt; I’ll lay out the ideas behind graph spectrum.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;First let’s discuss what spectrum mean&lt;/strong&gt;? Spectrum is a collection of atomic elements  (say in form of a set or vector or other mathematical object) to represent objects belonging to the same class (here class of objects could be an electrical signal, an audio signal or a chemical compound). For example, time series signal has a frequency spectrum. The importance of frequency spectrum is that one can recover the whole original signal from its frequency spectrum. Secondly from learning point of view, spectrum gives us a &lt;span style=&quot;color:#900C3F&quot;&gt;convenient and rich&lt;/span&gt; way to compare the &lt;span style=&quot;color:#900C3F&quot;&gt;similarity&lt;/span&gt; between two signals.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;span style=&quot;color:#900C3F&quot;&gt;&lt;strong&gt;Now, the question is does graph has a spectrum? And if so, what are its atomic elements?&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The most common graph spectrum you may heard of is &lt;span style=&quot;color:#D35400&quot;&gt;graph Laplacian eigenvalue spectrum&lt;/span&gt;, where atomic elements are eigenvalues of graph Laplacian matrix. There are two more powerful graph spectrums based on group theory: &lt;span style=&quot;color:#24A0AF&quot;&gt;Skew Spectrum&lt;/span&gt; and &lt;span style=&quot;color:#6AB234&quot;&gt;Graphlet Spectrum&lt;/span&gt;. The best thing about graph spectrum is that it atomic elements are solely depends upon the structure of the graph and thus invariant to permutation of graph vertex labels – a very important property desired from learning point of view.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We propose a new graph spectrum which is conceptually simple to understand but at the same time powerful enough as I’ll try to convince. &lt;strong&gt;Our Idea&lt;/strong&gt;: &lt;span style=&quot;color:#900C3F&quot;&gt;&lt;em&gt;Graph atomic structure (or spectrum) is encoded in the multiset of all nodes pairwise distances.&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;
  &lt;div style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/images/graph_spectrum_example.png&quot; style=&quot;width: 50%; height: 50%&quot; /&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;div style=&quot;text-align:center&quot;&gt;&lt;span style=&quot;color:#24A0AF&quot;&gt;&lt;b&gt;But &lt;span style=&quot;color:#EC7063&quot;&gt;what distance&lt;/span&gt; one should consider on a &lt;span style=&quot;color:#D4AC0D&quot;&gt;Graph&lt;/span&gt;?&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;div style=&quot;text-align:center&quot;&gt;&lt;span style=&quot;color:#900C3F&quot;&gt;&lt;b&gt;Welcome to The Family of Graph Spectral Distances (FGSD)&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;
  &lt;p&gt;FGSD is based on graph Laplacian &lt;script type=&quot;math/tex&quot;&gt;(L=D-W)&lt;/script&gt; spectral properties. Here &lt;script type=&quot;math/tex&quot;&gt;\phi_{k}&lt;/script&gt; &amp;amp; &lt;script type=&quot;math/tex&quot;&gt;\lambda_k&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;k^{th}&lt;/script&gt; Laplacian eigenvector and eigenvalue respectively.&lt;/p&gt;
  &lt;div style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/images/fgsd_equation.png&quot; style=&quot;width: 50%; height: 50%&quot; /&gt;&lt;/div&gt;
  &lt;div style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/images/fgsd_equation_fig.png&quot; style=&quot;width: 50%; height: 50%&quot; /&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;So what’s  special about this family of distance? Depending upon &lt;script type=&quot;math/tex&quot;&gt;f(\lambda)&lt;/script&gt;, FGSD can capture different type of information about graph sub-structures.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;FGSD Elements Encode Local Structure Information&lt;/strong&gt;: For  &lt;script type=&quot;math/tex&quot;&gt;f(\lambda)=\lambda^p&lt;/script&gt;  (&lt;script type=&quot;math/tex&quot;&gt;p\geq 1&lt;/script&gt;), one can show that &lt;script type=&quot;math/tex&quot;&gt;\mathcal{S}_f(x,y)&lt;/script&gt; takes only &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;-hop local neighborhood information. &lt;br /&gt;
&lt;strong&gt;FGSD Elements Encode Global Structure Information&lt;/strong&gt;: For  &lt;script type=&quot;math/tex&quot;&gt;f(\lambda)=\frac{1}{\lambda^p}&lt;/script&gt;  (&lt;script type=&quot;math/tex&quot;&gt;p\geq 1&lt;/script&gt;), one can show that &lt;script type=&quot;math/tex&quot;&gt;\mathcal{S}_f(x,y)&lt;/script&gt; captures global structure information.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Some Known Graph Distances Can Be Derived From FGSD.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Setting &lt;script type=&quot;math/tex&quot;&gt;f(\lambda)=\frac{1}{\lambda}&lt;/script&gt;  yields is harmonic or effective resistance distance.&lt;/li&gt;
    &lt;li&gt;Setting &lt;script type=&quot;math/tex&quot;&gt;f(\lambda)=\frac{1}{\lambda^2}&lt;/script&gt;  yields biharmonic distance.&lt;/li&gt;
    &lt;li&gt;Setting &lt;script type=&quot;math/tex&quot;&gt;f(\lambda)=e^{-2 \lambda}&lt;/script&gt;  yields heat diffusion distance.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;div style=&quot;text-align:center&quot;&gt;&lt;b&gt; Hunt for the best $$f(\lambda)$$ function that can exhibit ideal graph spectrum properties! &lt;/b&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 1 (Uniqueness of FGSD)&lt;/strong&gt;: The &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;-spectral distance matrix &lt;script type=&quot;math/tex&quot;&gt;\mathcal{S}_f=[ \mathcal{S}_f(x,y)]&lt;/script&gt;  uniquely determines the underlying graph (up to graph isomorphism), and each graph  has a unique &lt;script type=&quot;math/tex&quot;&gt;\mathcal{S}_f&lt;/script&gt; (up to permutation).  More precisely,  two undirected, weighted (and connected) graphs  &lt;script type=&quot;math/tex&quot;&gt;G_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;G_2&lt;/script&gt;  have the same FGSD based  distance matrix up to permutation,   i.e., &lt;script type=&quot;math/tex&quot;&gt;\mathcal{S}_{G_1}=P\mathcal{S}_{G_2}P^{T}&lt;/script&gt; for some permutation matrix &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt;,  if and only if  the two graphs are isomorphic.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Implications&lt;/strong&gt;:  One of the consequence of Theorem &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; is that the &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}&lt;/script&gt; based on multiset of FGSD is invariant under the
permutation of graph vertex labels. Unfortunately, it is possible that the multiset of
some FGSD members can be same for non-isomorphic graphs (otherwise, we would have a &lt;script type=&quot;math/tex&quot;&gt;\mathcal{O}(N^2)&lt;/script&gt; polynomial time algorithm for solving graph isomorphism problem!). However, there is lot of &lt;strong&gt;HOPE&lt;/strong&gt;. It is known that all non-isomorphic graphs with less than nine vertices have unique multisets of harmonic distance. While, for nine &amp;amp; ten vertex (simple) graphs, we have exactly &lt;script type=&quot;math/tex&quot;&gt;11&lt;/script&gt; &amp;amp; &lt;script type=&quot;math/tex&quot;&gt;49&lt;/script&gt; pairs of non-isomorphic
graphs (out of total &lt;script type=&quot;math/tex&quot;&gt;274,668&lt;/script&gt; &amp;amp; &lt;script type=&quot;math/tex&quot;&gt;12,005,168&lt;/script&gt; graphs) with the same harmonic spectra. These examples
show that there are significantly very low numbers of non-unique harmonic spectrums. Moreover, we
empirically found that the biharmonic distance has all unique multisets for at-least upto ten vertices
(&lt;script type=&quot;math/tex&quot;&gt;~11&lt;/script&gt; million graphs) and we couldn’t find any non-isomorphic graphs with the same biharmonic
multisets. Further, we have the following theorem regarding the uniqueness of &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}&lt;/script&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 2 (Uniqueness of  Graph Harmonic Spectrum)&lt;/strong&gt;:  Let &lt;script type=&quot;math/tex&quot;&gt;G=(V,E,W)&lt;/script&gt; be a graph of size &lt;script type=&quot;math/tex&quot;&gt;\vert V\vert&lt;/script&gt; with an unweighted adjacency matrix &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt;. Then, if two graphs &lt;script type=&quot;math/tex&quot;&gt;G_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;G_2&lt;/script&gt; have the same number of nodes but different number of edges, i.e, &lt;script type=&quot;math/tex&quot;&gt;\vert V_1 \vert =\vert V_2 \vert&lt;/script&gt; but &lt;script type=&quot;math/tex&quot;&gt;\vert E_1 \vert \neq  \vert E_2 \vert&lt;/script&gt;, then with respect to the harmonic distance multiset, &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(G_1) \neq \mathcal{R}(G_2)&lt;/script&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 3 (Eigenfunction Stability of FGSD)&lt;/strong&gt;: Let &lt;script type=&quot;math/tex&quot;&gt;\Delta \mathcal{S}_{xy}&lt;/script&gt; be the change in &lt;script type=&quot;math/tex&quot;&gt;\mathcal{S}_f(x,y)&lt;/script&gt; distance with respect to &lt;script type=&quot;math/tex&quot;&gt;\Delta w&lt;/script&gt; change  in  weight of  any  single edge on the graph. Then,   &lt;script type=&quot;math/tex&quot;&gt;\Delta  \mathcal{S}_{xy}&lt;/script&gt; for any vertex pair &lt;script type=&quot;math/tex&quot;&gt;(x,y)&lt;/script&gt; is bounded with respect  to the function of eigenvalue as follows,
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{equation*}  
\begin{split}
\Delta  \mathcal{S}_{xy} &amp; \leq 2   \big(|f(\lambda_{N-1}+2\triangle w ) -f(\lambda_{1})| \\
\end{split}
\end{equation*} %]]&gt;&lt;/script&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Implications&lt;/strong&gt;: Decreasing function of &lt;script type=&quot;math/tex&quot;&gt;f(\lambda)&lt;/script&gt; (such as &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{\lambda^p}&lt;/script&gt;) are more stable.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Conjecture (Sparsity of FGSD Graph Spectrum)&lt;/strong&gt;:  For any graph &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt;, let &lt;script type=&quot;math/tex&quot;&gt;\big|\mathcal{R}(f(\lambda))\big|_{G}&lt;/script&gt; represents the number of unique elements present in the multiset of &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}&lt;/script&gt;,  computed on an unweighted graph &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; based on some monotonic decreasing &lt;script type=&quot;math/tex&quot;&gt;f(\lambda)&lt;/script&gt;  function. Then, the following holds,
&lt;script type=&quot;math/tex&quot;&gt;\begin{equation*}
 \Big|\mathcal{R}(f(\lambda))\Big|_{G} \geq \Big|\mathcal{R}\Big(\frac{1}{\lambda}\Big)\Big|_{G}+2 
\end{equation*}&lt;/script&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Implications&lt;/strong&gt;: Harmonic Spectrum &lt;script type=&quot;math/tex&quot;&gt;(f(\frac{1}{\lambda}))&lt;/script&gt; produce the most sparse features.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;div style=&quot;text-align:center&quot;&gt;&lt;span style=&quot;color:#24A0AF&quot;&gt;&lt;b&gt;Feature Space of Harmonic and Biharmonic Spectrum&lt;/b&gt;&lt;/span&gt;&lt;/div&gt;

  &lt;div style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/images/feature_space.png&quot; style=&quot;width: 80%; height: 50%&quot; /&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Lastly, one can show that the worst case complexity of computing FGSD is &lt;script type=&quot;math/tex&quot;&gt;\mathcal{O}(N^2)&lt;/script&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;In the end, we take the histogram of FGSD spectrum to construct the graph feature vector&lt;/strong&gt;. Overall our hunt leads to the harmonic distance as an ideal member of FGSD family for extracting graph features.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Saurabh Verma, Zhi-Li Zhang. Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs. In 31st Conference on Neural Information Processing Systems (NIPS 2017). &lt;a href=&quot;http://www-users.cs.umn.edu/~verma/NIPS17-FGSD-verma.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 21 Oct 2017 19:01:00 -0500</pubDate>
        <link>http://localhost:4000/tutorials/2017/10/21/FGSD.html</link>
        <guid isPermaLink="true">http://localhost:4000/tutorials/2017/10/21/FGSD.html</guid>
        
        
        <category>Tutorials</category>
        
      </item>
    
      <item>
        <title> Blog: Graph Convolutional Networks.</title>
        <description>
</description>
        <pubDate>Fri, 08 Sep 2017 19:01:30 -0500</pubDate>
        <link>http://localhost:4000/tutorials/2017/09/08/hide.html</link>
        <guid isPermaLink="true">http://localhost:4000/tutorials/2017/09/08/hide.html</guid>
        
        
        <category>Tutorials</category>
        
      </item>
    
      <item>
        <title> Blog: Stability of Learning Algorithms Part 3.</title>
        <description>&lt;p&gt;&lt;strong&gt;Extending to Ranking Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;\mathbf{z}=(\mathbf{x},y)&lt;/script&gt; be a sample such that &lt;script type=&quot;math/tex&quot;&gt;y\in \mathbb{R}&lt;/script&gt; represents the ranking score. If &lt;script type=&quot;math/tex&quot;&gt;y_1&gt;y_2&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;\mathrm{rank}(\mathbf{x}_1)&gt;\mathrm{rank}(\mathbf{x}_2)&lt;/script&gt;. Let &lt;script type=&quot;math/tex&quot;&gt;A_{S}&lt;/script&gt; be such a ranking score learning function.&lt;/p&gt;

&lt;p&gt;We define loss function on a pairwise sample &lt;script type=&quot;math/tex&quot;&gt;(\mathbf{z},\mathbf{z}^{'})&lt;/script&gt; and compute a real-valued loss i.e., &lt;script type=&quot;math/tex&quot;&gt;\ell:\mathbf{Z}^{m} \times \mathbf{Z}\times \mathbf{Z} \rightarrow \mathbb{R}&lt;/script&gt;. We also require our loss function to be symmetric on pairwise sample i.e, &lt;script type=&quot;math/tex&quot;&gt;\ell(A_S,\mathbf{z},\mathbf{z}^{'})=\ell(A_S,\mathbf{z}^{'},\mathbf{z})&lt;/script&gt; and bounded as follows, &lt;script type=&quot;math/tex&quot;&gt;0\leq \ell(A_S,\mathbf{z},\mathbf{z}^{'}) \leq M&lt;/script&gt; for ranking problem.&lt;/p&gt;

&lt;p&gt;We define the generalization error or risk &lt;script type=&quot;math/tex&quot;&gt;R(A_S)&lt;/script&gt; as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R(A_S):=\mathbf{E}_{z,z^{'}}[\ell(A_S,\mathbf{z},\mathbf{z}^{'})]=\int \ell(A_S,\mathbf{z},\mathbf{z}^{'}) p(\mathbf{z},\mathbf{z}^{'}) d\mathbf{z}d\mathbf{z}^{'}&lt;/script&gt;

&lt;p&gt;Empirical error &lt;script type=&quot;math/tex&quot;&gt;R_{emp}(A_S)&lt;/script&gt; is defined as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_{emp}(A_S):=\frac{1}{\binom{m}{2}}\sum\limits_{i=1}^{m-1}\sum\limits_{j=i+1}^{m}\ell(A_S,\mathbf{z}_i,\mathbf{z}_j)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Let’s assume that our algorithm &lt;script type=&quot;math/tex&quot;&gt;A_S&lt;/script&gt; has uniform stability &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;, i.e.  it satisfies &lt;script type=&quot;math/tex&quot;&gt;\forall i\in\{1,m\}&lt;/script&gt;,&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\underset{S,z,z^{'}}{\sup}|\ell(A_S,\mathbf{z},\mathbf{z}^{'}) -\ell(A_{S^{i}},\mathbf{z},\mathbf{z}^{'})| \leq \beta&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The definition here, involves &lt;script type=&quot;math/tex&quot;&gt;S^{i}&lt;/script&gt; rather than &lt;script type=&quot;math/tex&quot;&gt;S^{\backslash i}&lt;/script&gt; which is slightly different than uniform stability definitions.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
|R(A_S)-R(A_{S^{i}})| &amp; = \Big|\mathbf{E}_{z,z^{'}}[\ell(A_S,\mathbf{z},\mathbf{z}^{'})] - \mathbf{E}_{z,z^{'}}[\ell(A_{S^{i}},\mathbf{z},\mathbf{z}^{'})] \Big|  \\
&amp;=\Big| \mathbf{E}_{z,z^{'}}[\ell(A_S,\mathbf{z},\mathbf{z}^{'})-\ell(A_{S^{i}},\mathbf{z},\mathbf{z}^{'})] \Big| \\
&amp; \leq \beta \\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
|R_{emp}(A_S)-R_{emp}(A_{S^{k}})| &amp; =  \frac{1}{\binom{m}{2}}\sum\limits_{i=1,i\neq k}^{m-1}\sum\limits_{j=i+1,j\neq k}^{m}\Bigg|\Big(\ell(A_S,\mathbf{z}_i,\mathbf{z}_j) - \ell(A_{S^{k}},\mathbf{z}_i,\mathbf{z}_j) \Big) \Bigg| + \\
&amp; \hspace{1.5em} \frac{1}{\binom{m}{2}}\sum\limits_{i=1,i \neq k}^{m}\Bigg|\Big(\ell(A_S,\mathbf{z}_i,\mathbf{z}_k) - \ell(A_{S^{k}},\mathbf{z}_i,\mathbf{z}_k) \Big) \Bigg|  \\
&amp; \leq  \frac{1}{\binom{m}{2}}\Big(\binom{m}{2}-(m-1)\Big)\beta + (m-1) M \\
&amp; &lt; \beta+\frac{2M}{m}
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
|\Big(R(A_S)- R_{emp}(A_S)\Big)-\Big(R(A_{S^{i}})-R_{emp}(A_{S^{i}})\Big)| &amp; \leq |R(A_S)-R(A_{S^{i}})|+|R_{emp}(A_S)-R_{emp}(A_{S^{i}})| \\
&amp; \leq 2\beta+\frac{2M}{m} \\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
\mathbf{E}_{S}[R(A_S)] &amp;= \mathbf{E}_{S,z_{i}^{'},z_{j}^{'}}[\ell(A_{S},\mathbf{z}_{i}^{'},\mathbf{z}_{j}^{'}) \\
\mathbf{E}_{S}[R_{emp}(A_S)] &amp;= \mathbf{E}_{S,z_{i}^{'},z_{j}^{'}}[\ell(A_{S^{i,j}},\mathbf{z}_{i}^{'},\mathbf{z}_{j}^{'})] \\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
\mathbf{E}_{S}[R(A_S)-R_{emp}(A_S)] &amp;= \mathbf{E}_{S,z_{i}^{'},z_{j}^{'}}[\ell(A_S,\mathbf{z}_{i}^{'},\mathbf{z}_{j}^{'})-\ell(A_{S^{i,j}},\mathbf{z}_{i}^{'},\mathbf{z}_{j}^{'})] \\
&amp; \leq \mathbf{E}_{S,z_{i}^{'},z_{j}^{'}}[|\ell(A_S,\mathbf{z}_{i}^{'},\mathbf{z}_{j}^{'})-\ell(A_{S^{i}},\mathbf{z}_{i}^{'},\mathbf{z}_{j}^{'}) + \\
&amp; \hspace{1.5em} \ell(A_{S^{i}},\mathbf{z}_{i}^{'},\mathbf{z}_{j}^{'})   -\ell(A_{S^{i,j}},\mathbf{z}_{i}^{'},\mathbf{z}_{j}^{'}) |] \\
&amp; \leq \mathbf{E}_{S,z_{i}^{'},z_{j}^{'}}[|\ell(A_S,\mathbf{z}_{i}^{'},\mathbf{z}_{j}^{'})-\ell(A_{S^{i}},\mathbf{z}_{i}^{'},\mathbf{z}_{j}^{'})|] +\\
&amp; \hspace{1.5em} \mathbf{E}_{S,z_{i}^{'},z_{j}^{'}}[|\ell(A_{S^{i}},\mathbf{z}_{i}^{'},\mathbf{z}_{j}^{'})   -\ell(A_{S^{i,j}},\mathbf{z}_{i}^{'},\mathbf{z}_{j}^{'}) |] \\
&amp; \leq 2\beta \\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Applying McDiarmid’s inequality to derive generalization bounds for uniform stable algorithms:&lt;/p&gt;

  &lt;center&gt;$$ \mathbf{P}\Bigg( \Big(R(A_S)-R_{emp}(A_S)\Big)  - 2\beta \geq   \epsilon \Bigg)  \leq \underbrace{e^{-\frac{2\epsilon^2}{m(2\beta+\frac{2M}{m})^2}}}_{\delta} $$
$$ \implies    \mathbf{P}\Bigg( \Big(R(A_S)-R_{emp}(A_S)\Big) \leq  2\beta +  (m\beta+M)\sqrt{\frac{2\log \frac{1}{\delta}}{m}} \Bigg) \geq 1-\delta $$&lt;/center&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Tue, 17 Jan 2017 18:02:00 -0600</pubDate>
        <link>http://localhost:4000/tutorials/2017/01/17/Stability-Machine-Learning-Part3.html</link>
        <guid isPermaLink="true">http://localhost:4000/tutorials/2017/01/17/Stability-Machine-Learning-Part3.html</guid>
        
        
        <category>Tutorials</category>
        
      </item>
    
      <item>
        <title> Blog: Stability of Learning Algorithms Part 2.</title>
        <description>&lt;p&gt;&lt;strong&gt;Extending to Randomized Algorithms&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt; be  a set of random parameters.&lt;/p&gt;

&lt;p&gt;Let’s assume that our algorithm &lt;script type=&quot;math/tex&quot;&gt;A_{(S,R)}&lt;/script&gt; has uniform stability &lt;script type=&quot;math/tex&quot;&gt;(\beta,\rho)&lt;/script&gt;, if  it satisfies,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underset{S,z}{\sup}|\mathbf{E}_{R}[\ell(A_{(S,R)},\mathbf{z})] -\mathbf{E}_{R}[\ell(A_{(S^{\backslash i},R)},\mathbf{z})]| \leq \beta \hspace{3em} \forall i\in\{1,m\}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underset{S,R,r_i^{'},z}{\sup}| \ell(A_{(S,R)},\mathbf{z}) -\ell(A_{(S,R^{i})},\mathbf{z})| \leq \rho \hspace{3em} \forall i\in\{1,T\}&lt;/script&gt;

&lt;!---
&gt;$$\underset{r_1,...,r_T,r_i^{'},z}{\sup}| \ell(A_{S,(r_1,...,r_T)},\mathbf{z}) -\ell(A_{S,(r_1,..,r_{i-1},r_i^{'},r_{i+1},..,r_T)},\mathbf{z})| \leq \rho $$
--&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K_{(S,R)}=R(A_{(S,R)})-R_{emp}(A_{(S,R)})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
|K_{(S,R)}-K_{(S,R^{i})}|&amp;=\Big| \big(R(A_{(S,R)})-R_{emp}(A_{(S,R)}) \big) - \big( R(A_{(S,R^{i})}) - R_{emp}(A_{(S,R^{i})})\big) \Big|\\
&amp;=  \Big| \mathbf{E}_z [\ell(A_{(S,R)},\mathbf{z})- \ell(A_{(S,R^{i})},\mathbf{z})]-\frac{1}{m}\sum\limits_{i=1}^{m} (\ell(A_{(S,R)},\mathbf{z}_i)- \ell(A_{(S,R^{i})},\mathbf{z}_i))     \Big|\\
&amp;\leq \mathbf{E}_z [|\ell(A_{(S,R)},\mathbf{z})- \ell(A_{(S,R^{i})},\mathbf{z})|]+\frac{1}{m} \sum\limits_{i=1}^{m}|(\ell(A_{(S,R)},\mathbf{z}_i)- \ell(A_{(S,R^{i})},\mathbf{z}_i))| \\
&amp; \leq 2\rho \\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Applying McDiarmid’s inequality assuming &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; is fixed, we get (Event 1),&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P\Bigg(  K_{(S,R)}-\mathbf{E}_R[K_{(S,R)}] \geq \epsilon \Big|  S  \Bigg) \leq \underbrace{e^{-\frac{\epsilon^2}{2T\rho^2}}}_{\delta}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P\Bigg(  K_{(S,R)}-\mathbf{E}_R[K_{(S,R)}] \leq  \rho\sqrt{2T} \sqrt{\log \big(\frac{1}{\delta} \big)}  \Big|   S  \Bigg) \geq 1-\delta&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
\mathbf{E}_{S,R}[K_{(S,R)}] &amp;= \mathbf{E}_{S,R}[R(A_{(S,R)})] - \mathbf{E}_{S,R}[R_{emp}(A_{(S,R)})]  \\
&amp;= \mathbf{E}_{S,R}[\mathbf{E}_z[\ell(A_{(S,R)},\mathbf{z})]] - \mathbf{E}_{S,R}[\frac{1}{m}\sum\limits_{j=1}^{m}\ell(A_{(S,R)},\mathbf{z}_j)]  \\
&amp; = \mathbf{E}_{S,z}[\mathbf{E}_R[\ell(A_{(S,R)},\mathbf{z})]] - \mathbf{E}_{S} [\mathbf{E}_R[\ell(A_{(S,R)},\mathbf{z}_j)]]\\
&amp; = \mathbf{E}_{S,z}[\mathbf{E}_R[\ell(A_{(S,R)},\mathbf{z})]]- \mathbf{E}_{S,z}[\mathbf{E}_R[\ell(A_{(S^{\backslash i},R)},\mathbf{z})]] + \mathbf{E}_{S,z}[\mathbf{E}_R[\ell(A_{(S^{\backslash i},R)},\mathbf{z})]] - \\
&amp; \hspace{3em} \mathbf{E}_{S} [\mathbf{E}_R[\ell(A_{(S^{\backslash i},R)},\mathbf{z}_j)]]+\mathbf{E}_{S} [\mathbf{E}_R[\ell(A_{(S^{\backslash i},R)},\mathbf{z}_j)]] - \mathbf{E}_{S} [\mathbf{E}_R[\ell(A_{(S,R)},\mathbf{z}_j)]]\\
&amp; = \mathbf{E}_{S,z}[\mathbf{E}_R[\ell(A_{(S,R)},\mathbf{z})] -  \mathbf{E}_R[\ell(A_{(S^{\backslash i},R)},\mathbf{z})]] + \mathbf{E}_{S,z}[\mathbf{E}_R[\ell(A_{(S^{\backslash i},R)},\mathbf{z})]] -\\
&amp; \hspace{3em} \mathbf{E}_{S} [\mathbf{E}_R[\ell(A_{(S^{\backslash i},R)},\mathbf{z}_j)]] + \mathbf{E}_{S} [\mathbf{E}_R[\ell(A_{(S^{\backslash i},R)},\mathbf{z}_j)]-\mathbf{E}_R[\ell(A_{(S,R)},\mathbf{z}_j)]]\\
&amp; \leq 2\beta + \mathbf{E}_{S,z}[\mathbf{E}_R[\ell(A_{(S^{\backslash i},R)},\mathbf{z})]] - \mathbf{E}_{S} [\mathbf{E}_R[\ell(A_{(S^{\backslash i},R)},\mathbf{z}_j)]]\\
&amp; \leq 2\beta + \mathbf{E}_{S,z_j}[\mathbf{E}_R[\ell(A_{(S^{\backslash i},R)},\mathbf{z}_j)]] - \mathbf{E}_{S} [\mathbf{E}_R[\ell(A_{(S^{\backslash i},R)},\mathbf{z}_j)]]\\
&amp; \leq 2\beta + \mathbf{E}_{S}[\mathbf{E}_R[\ell(A_{(S^{\backslash i},R)},\mathbf{z}_j)]] - \mathbf{E}_{S} [\mathbf{E}_R[\ell(A_{(S^{\backslash i},R)},\mathbf{z}_j)]]\\
&amp; = 2\beta \\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;!---

$$\begin{equation}
\begin{split}
\underset{S}{\sup} \Big|\mathbf{E}_R[K_{(S,R)}]-\mathbf{E}_R[K_{(S^{i},R)}]\Big|&amp;=\underset{S}{\sup}\Big| \mathbf{E}_R[K_{(S,R)}]-\mathbf{E}_R[K_{(S^{\backslash i},R)}] + \\
&amp; \hspace{2em} \mathbf{E}_R[K_{(S^{\backslash i},R)}] -\mathbf{E}_R[K_{(S^{i},R)}]  \Big|  \\
\end{split}
\end{equation}$$

$$\begin{equation}
\begin{split}
&amp; = \underset{S}{\sup}\Big|  \Big|\\
\end{split}
\end{equation}$$


---&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
|\mathbf{E}_R[K_{(S,R)}]-\mathbf{E}_R[K_{(S^{i},R)}]|&amp;=\Big| \mathbf{E}_R[\big(R(A_{(S,R)})-R_{emp}(A_{(S,R)}) \big)] - \mathbf{E}_R[\big( R(A_{(S^{i},R)}) - R_{emp}(A_{(S^{i},R)})\big)] \Big|\\
&amp; \leq \Big| \mathbf{E}_R[R(A_{(S,R)})]-\mathbf{E}_R[R(A_{(S^{i},R)})] \Big| +\Big| \mathbf{E}_R[R_{emp}(A_{(S,R)})]-\mathbf{E}_R[R_{emp}(A_{(S^{i},R)})] \Big|\\
&amp; \leq 2\beta+(2\beta+\frac{M}{m}) \\
&amp; \leq 4\beta+\frac{M}{m} \\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Applying McDiarmid’s inequality, we get (Event 2),&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P \Bigg(  \mathbf{E}_R[K_{(S,R)}]-\mathbf{E}_S[\mathbf{E}_R[K_{(S,R)}]] \geq \epsilon   \Bigg) \leq \underbrace{e^{-\frac{2\epsilon^2}{m(4\beta+\frac{M}{m})^2}}}_{\delta}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P \Bigg(  \mathbf{E}_R[K_{(S,R)}] \leq  2\beta +  (4m\beta+M)\sqrt{\frac{\log \frac{1}{\delta}}{2m}} \Bigg) \geq 1-\delta&lt;/script&gt;

&lt;p&gt;Now, probability of holding both the events simultaneously is &lt;script type=&quot;math/tex&quot;&gt;1-2\delta&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{P}\Bigg( R(A_{(S,R)})-R_{emp}(A_{(S,R)})  \leq  2\beta +  (4m\beta+M)\sqrt{\frac{\log \frac{1}{\delta}}{2m}} +  \rho\sqrt{2T} \sqrt{\log \big(\frac{1}{\delta} \big)} \Bigg) \geq 1-2\delta&lt;/script&gt;

&lt;p&gt;Replacing &lt;script type=&quot;math/tex&quot;&gt;\delta&lt;/script&gt; by &lt;script type=&quot;math/tex&quot;&gt;\frac{\delta}{2}&lt;/script&gt;, we get,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{P}\Bigg( \ R(A_{(S,R)})-R_{emp}(A_{(S,R)})  \leq  2\beta +  \Big( \frac{4m\beta+M}{\sqrt{2m}} +  \rho\sqrt{2T} \Big) \sqrt{\log \big(\frac{2}{\delta} \big) }   \Bigg) \geq 1-\delta&lt;/script&gt;
&lt;/blockquote&gt;

</description>
        <pubDate>Tue, 17 Jan 2017 18:01:30 -0600</pubDate>
        <link>http://localhost:4000/tutorials/2017/01/17/Stability-Machine-Learning-Part2.html</link>
        <guid isPermaLink="true">http://localhost:4000/tutorials/2017/01/17/Stability-Machine-Learning-Part2.html</guid>
        
        
        <category>Tutorials</category>
        
      </item>
    
      <item>
        <title> Blog: Stability of Learning Algorithms Part 1.</title>
        <description>&lt;p&gt;This is walk-through on how  to derive generalization bounds based on the stability of algorithms. Understanding the proof helps  to extend or modify the results according to your needs. For instance, I had a loss function which was unbounded, and since to apply generalization results your loss function must be bounded, I was stuck. So, this my effort  to state the proof as clear as possible along with the assumptions needed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;: Let &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X} \in \mathbb{R}^d, \mathbf{Y} \in \mathbb{R}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; be a training set &lt;script type=&quot;math/tex&quot;&gt;S=\{\mathbf{z}_1=(\mathbf{x}_1,y_1),\mathbf{z}_2=(\mathbf{x}_2,y_2),...,\mathbf{z}_m=(\mathbf{x}_m,y_m)\}&lt;/script&gt;. Let &lt;script type=&quot;math/tex&quot;&gt;A_S&lt;/script&gt; be a symmetric learning algorithm i.e.  does not depend on the order of the data points in the training set. We define the following two more notations as follows:&lt;/p&gt;

&lt;p&gt;Removing &lt;script type=&quot;math/tex&quot;&gt;i^{th}&lt;/script&gt; data point in the set &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S^{\backslash i}=\{\mathbf{z}_1,....,\mathbf{z}_{i-1},\mathbf{z}_{i+1},.....,\mathbf{z}_m \}&lt;/script&gt;

&lt;p&gt;Replacing &lt;script type=&quot;math/tex&quot;&gt;i^{th}&lt;/script&gt; data point in the set &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; by &lt;script type=&quot;math/tex&quot;&gt;\mathbf{z}_{i}^{'}&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S^{i}=\{\mathbf{z}_1,....,\mathbf{z}_{i-1},\mathbf{z}_{i}^{'},\mathbf{z}_{i+1},.....,\mathbf{z}_m \}&lt;/script&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; be an unknown distribution from which &lt;script type=&quot;math/tex&quot;&gt;\mathbf{z}_1,....,\mathbf{z}_m&lt;/script&gt; data points are sampled to form a training set &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;. Here, we assume all samples (including the replacement sample) are i.i.d. unless mentioned otherwise. Let &lt;script type=&quot;math/tex&quot;&gt;\mathbf{E}_S[f]&lt;/script&gt; denotes the expectation of the function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; when  &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;  samples are drawn from the distribution &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; to form &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;. Similarly, let &lt;script type=&quot;math/tex&quot;&gt;\mathbf{E}_z[f]&lt;/script&gt; denotes the expectation of the function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; when  &lt;script type=&quot;math/tex&quot;&gt;\mathbf{z}&lt;/script&gt; is sampled  according to the distribution &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We define the generalization error or risk &lt;script type=&quot;math/tex&quot;&gt;R(A_S)&lt;/script&gt; as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R(A_S)=\mathbf{E}_z[\ell(A_S,\mathbf{z})]=\int \ell(A_S,\mathbf{z}) p(\mathbf{z}) d\mathbf{z}&lt;/script&gt;

&lt;p&gt;Empirical error &lt;script type=&quot;math/tex&quot;&gt;R_{emp}(A_S)&lt;/script&gt; is defined as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_{emp}(A_S):=\frac{1}{m}\sum\limits_{j=1}^{m}\ell(A_S,\mathbf{z}_j)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\implies R_{emp}(A_{S^{  i}})=\frac{1}{m}\sum\limits_{j=1, j \neq i}^{m}\ell(A_{S^{ i}},\mathbf{z}_j)+\frac{1}{m}\ell(A_{S^{ i}},\mathbf{z}_i^{'})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_{emp}(A_{S^{\backslash i}}):=\frac{1}{m}\sum\limits_{j=1, j \neq i}^{m}\ell(A_{S^{\backslash i}},\mathbf{z}_j)&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Generalization Bounds based on Stability of Learning Algorithms&lt;/strong&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Let’s assume that our algorithm &lt;script type=&quot;math/tex&quot;&gt;A_S&lt;/script&gt; has uniform stability &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;, i.e.  it satisfies &lt;script type=&quot;math/tex&quot;&gt;\forall i\in\{1,m\}&lt;/script&gt;,&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\underset{S,z}{\sup}|\ell(A_S,\mathbf{z}) -\ell(A_{S^{\backslash i}},\mathbf{z})| \leq \beta&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;To derive generalization bounds for uniform stable algorithms, we are going to only use McDiarmid’s inequality. Let &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; be some set and &lt;script type=&quot;math/tex&quot;&gt;f:\mathbf{X}^{m} \rightarrow R&lt;/script&gt;, then inequality is given as (more detail &lt;a href=&quot;http://web.eecs.umich.edu/~cscott/past_courses/eecs598w14/notes/09_bounded_difference.pdf&quot;&gt;here&lt;/a&gt;),&lt;/p&gt;

&lt;blockquote&gt;
  &lt;script type=&quot;math/tex; mode=display&quot;&gt;if \underset{x_1,..,x_i,..,x_m,x_i^{'}}{\sup}|f(x_1,..,x_i,..,x_m)-f(x_1,..,x_i^{'},..,x_m)| \leq c_i&lt;/script&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;=\underset{x_1,..,x_i,..,x_m,x_i^{'}}{\sup}|f_S-f_{S^{i}}| \leq c_i \hspace{1em},\forall i&lt;/script&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\implies P\Big( f(S)-\mathbf{E}_S[f(S)] \geq \epsilon \Big) \leq e^{-\frac{2 \epsilon^2}{\sum_{i=1}^{m}c_i^2}}&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;Strategy is to bound &lt;script type=&quot;math/tex&quot;&gt;\vert f_S-f_{S^{i}} \vert&lt;/script&gt; using &lt;script type=&quot;math/tex&quot;&gt;\vert f_S-f_{S^{\backslash i}} \vert&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\vert f_{S^{i}}-f_{S^{\backslash i}} \vert&lt;/script&gt;.  We will derive some lemmas that would be helpful to compute variables needed for applying McDiarmid’s inequality.&lt;/p&gt;

&lt;p&gt;Since, samples are i.i.d., we have,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
\mathbf{E}_{S}[\ell(A_S,\mathbf{z})] &amp;=\int \ell\big(A(\mathbf{z}_1,...,\mathbf{z}_m),\mathbf{z}\big)p(\mathbf{z}_1,...,\mathbf{z}_m) d\mathbf{z}_1...d\mathbf{z}_m \\
&amp;=\int \ell\big(A(\mathbf{z}_1,...,\mathbf{z}_m),\mathbf{z}\big)p(\mathbf{z}_1)...p(\mathbf{z}_m)  d\mathbf{z}_1...d\mathbf{z}_m \\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
\mathbf{E}_{S}[\ell(A_S,\mathbf{z}_j)] &amp;=\int \ell\big(A(\mathbf{z}_1,..,\mathbf{z}_j,..,\mathbf{z}_m),\mathbf{z}_j\big)p(\mathbf{z}_1,..,\mathbf{z}_j,..,\mathbf{z}_m) d\mathbf{z}_1...d\mathbf{z}_m \\
&amp; = \int \ell\big(A(\mathbf{z}_1,..,\mathbf{z}_j,..,\mathbf{z}_m),\mathbf{z}_j\big)p(\mathbf{z}_1)..p(\mathbf{z}_j)..p(\mathbf{z}_m) d\mathbf{z}_1...d\mathbf{z}_m \\
&amp; = \int \ell\big(A(\mathbf{z}_1,..,\mathbf{z}_i^{'},..,\mathbf{z}_m),\mathbf{z}_i^{'}\big)p(\mathbf{z}_1)..p(\mathbf{z}_i^{'})..p(\mathbf{z}_m) d\mathbf{z}_1..d\mathbf{z}_i^{'}..d\mathbf{z}_m \\
&amp; = \int \ell\big(A(\mathbf{z}_1,..,\mathbf{z}_i^{'},..,\mathbf{z}_m),\mathbf{z}_i^{'}\big)p(\mathbf{z}_1,..,\mathbf{z}_i^{'},..,\mathbf{z}_m) d\mathbf{z}_1..d\mathbf{z}_i^{'}..d\mathbf{z}_m  \int p(\mathbf{z}_i) d\mathbf{z}_i\\
&amp; = \int \ell\big(A(\mathbf{z}_1,..,\mathbf{z}_i^{'},..,\mathbf{z}_m),\mathbf{z}_i^{'}\big)p(\mathbf{z}_1,..,\mathbf{z}_i,\mathbf{z}_i^{'},..,\mathbf{z}_m) d\mathbf{z}_1...d\mathbf{z}_m d\mathbf{z}_i^{'}\\
&amp;=\mathbf{E}_{S,z_{i}^{'}}[\ell(A_{S^{i}},\mathbf{z}_i^{'})]
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
\mathbf{E}_S[R(A_S)-R_{emp}(A_S)] &amp;= \mathbf{E}_{S}[\mathbf{E}_z[\ell(A_S,\mathbf{z})]]-\frac{1}{m}\sum\limits_{j=1}^{m}\mathbf{E}_{S}[\ell(A_S,\mathbf{z}_j)] \\
&amp;= \mathbf{E}_{S}[\mathbf{E}_z[\ell(A_S,\mathbf{z})]]-\mathbf{E}_{S}[\ell(A_S,\mathbf{z}_j)] \\
%&amp; =\mathbf{E}_{S}[\mathbf{E}_{z_{i}^{'}}[\ell(A_S,\mathbf{z}_{i}^{'})]] -    \mathbf{E}_{z_{i}^{'}}[\mathbf{E}_{S}[\ell(A_{S^{i}},\mathbf{z}_i^{'})]] \\
&amp; =\mathbf{E}_{S,z_{i}^{'}}[\ell(A_S,\mathbf{z}_{i}^{'})]]- \mathbf{E}_{S,z_{i}^{'}}[\ell(A_{S^{i}},\mathbf{z}_i^{'})]\\
&amp; =\mathbf{E}_{S,z_{i}^{'}}[\ell(A_S,\mathbf{z}_{i}^{'})-\ell(A_{S^{i}},\mathbf{z}_{i}^{'})] \\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
|R(A_S)-R(A_{S^{\backslash i}})|&amp;=  |\mathbf{E}_z[\ell(A_S,\mathbf{z})]-\mathbf{E}_z[\ell(A_{S^{\backslash i}},\mathbf{z})]| \\
&amp; =	|\mathbf{E}_z[\ell(A_S,\mathbf{z}) - \ell(A_{S^{\backslash i}},\mathbf{z})] |\\
&amp; \leq \mathbf{E}_z[|\ell(A_S,\mathbf{z})-\ell(A_{S^{\backslash i}},\mathbf{z})|] \\
&amp; \leq \mathbf{E}_z[\beta]=\beta \\
|R(A_{S^{i}})-R(A_{S^{\backslash i}})|&amp; \leq  \mathbf{E}_z[|\ell(A_{S^{i}},\mathbf{z})-\ell(A_{S^{\backslash i}},\mathbf{z})|] \\
&amp; \leq \mathbf{E}_z[\beta]=\beta \\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;!---
$$\begin{equation}
\begin{split}
|R_{emp}(A_S)-R_{emp}(A_{S^{\backslash i}})|&amp;= \Big|\frac{1}{m}\sum\limits_{j=1}^{m}\ell(A_S,\mathbf{z}_j)-\frac{1}{m}\sum\limits_{j=1,j \neq i}^{m}\ell(A_{S^{\backslash i}},\mathbf{z}_j) \Big|  \\
&amp; \leq \Big| \frac{1}{m}\sum\limits_{j=1,j \neq i}^{m} (\ell(A_S,\mathbf{z}_j)-\ell(A_{S^{\backslash i}},\mathbf{z}_j)) \Big|+\Big| \frac{1}{m}\ell(A_S,\mathbf{z}_i) \Big| \\
&amp; \leq \frac{(m-1)}{m}\beta  +\frac{M}{m} \\
&amp; \leq \beta  +\frac{M}{m} \\
\end{split}
\end{equation}$$
--&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
|R(A_S)-R(A_{S^{i}})|&amp; \leq|R(A_S)-R(A_{S^{\backslash i}})|+|R(A_{S^{\backslash i}}) - R(A_{S^{i}}) |\\
&amp; \leq 2\beta \\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
|R_{emp}(A_S)-R_{emp}(A_{S^{i}})| &amp; \leq |\frac{1}{m}\sum\limits_{j=1,j \neq i}^{m} (\ell(A_S,\mathbf{z}_j)-\ell(A_{S^{i}},\mathbf{z}_j))|+|\frac{1}{m} (\ell(A_S,\mathbf{z}_i)-\ell(A_{S^{i}},\mathbf{z}_i^{'})) |\\
&amp; \leq  |\frac{1}{m}\sum\limits_{j=1,j \neq i}^{m} (\ell(A_S,\mathbf{z}_j)-\ell(A_{S^{\backslash i}},\mathbf{z}_j))|+ |\frac{1}{m}\sum\limits_{j=1,j \neq i}^{m} (\ell(A_{S^{i}},\mathbf{z}_j)-\ell(A_{S^{\backslash i}},\mathbf{z}_j))|  + \\
&amp; |\frac{1}{m} (\ell(A_S,\mathbf{z}_i)-\ell(A_{S^{i}},\mathbf{z}_i^{'})) |\\
&amp; \leq  2\frac{(m-1)}{m}\beta +\frac{M}{m} \\
&amp; \leq   2\beta +\frac{M}{m} \\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note: &lt;script type=&quot;math/tex&quot;&gt;(\ell(A_S,\mathbf{z}_i)-\ell(A_{S^{i}},\mathbf{z}_i^{'}))&lt;/script&gt; introduces &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; (bound on loss) in the equation. If your loss is &lt;strong&gt;unbounded&lt;/strong&gt; then you may be able to bound &lt;script type=&quot;math/tex&quot;&gt;(\ell(A_S,\mathbf{z}_i)-\ell(A_{S^{i}},\mathbf{z}_i^{'}))&lt;/script&gt; and just replace &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; by it, in the generalization bound.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
|\big(R(A_S)-R_{emp}(A_{S})\big) -\big(R(A_{S^{i}})-R_{emp}(A_{S^{i}})\big)|&amp; \leq |R(A_S)-R(A_{S^{i}})| + |R_{emp}(A_S)-R_{emp}(A_{S^{i}})|\\
&amp;\leq 2\beta+2\beta+\frac{M}{m} \\
&amp; =4\beta+\frac{M}{m} \\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
\mathbf{E}_{S}[R(A_S)-R_{emp}(A_S)] &amp;= \mathbf{E}_{S,z_{i}^{'}}[\ell(A_S,\mathbf{z}_{i}^{'})-\ell(A_{S^{i}},\mathbf{z}_{i}^{'})] \\
&amp; \leq \mathbf{E}_{S,z_{i}^{'}}[|\ell(A_S,\mathbf{z}_{i}^{'})-\ell(A_{S^{i}},\mathbf{z}_{i}^{'})|]\\
&amp; \leq \mathbf{E}_{S,z_{i}^{'}}[|\ell(A_S,\mathbf{z}_{i}^{'})-\ell(A_{S^{\backslash i}},\mathbf{z}_{i}^{'})| + |\ell(A_{S^{i}},\mathbf{z}_{i}^{'})-\ell(A_{S^{\backslash i}},\mathbf{z}_{i}^{'})|]\\
&amp; \leq 2\beta \\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Applying McDiarmid’s inequality to derive generalization bounds for uniform stable algorithms:&lt;/p&gt;

  &lt;center&gt;$$ \mathbf{P}\Bigg( \Big(R(A_S)-R_{emp}(A_S)\Big)  - 2\beta \geq   \epsilon \Bigg)  \leq \underbrace{e^{-\frac{2\epsilon^2}{m(4\beta+\frac{M}{m})^2}}}_{\delta} $$
$$ \implies    \mathbf{P}\Bigg( \Big(R(A_S)-R_{emp}(A_S)\Big) \leq  2\beta +  (4m\beta+M)\sqrt{\frac{\log \frac{1}{\delta}}{2m}} \Bigg) \geq 1-\delta $$&lt;/center&gt;
&lt;/blockquote&gt;

</description>
        <pubDate>Tue, 17 Jan 2017 18:01:00 -0600</pubDate>
        <link>http://localhost:4000/tutorials/2017/01/17/Stability-Machine-Learning.html</link>
        <guid isPermaLink="true">http://localhost:4000/tutorials/2017/01/17/Stability-Machine-Learning.html</guid>
        
        
        <category>Tutorials</category>
        
      </item>
    
      <item>
        <title> Blog: Dirichlet Distributions.</title>
        <description>&lt;p&gt;Dirichlet Distribution is &lt;em&gt;a distribution over distributions&lt;/em&gt;. More specifically, it is a distribution over &lt;em&gt;pmfs (probability mass functions)&lt;/em&gt;. You can imagine, as if there is a bag of &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; dices, and each dice has a corresponding pmf (related to six possible outcomes). Now picking a dice is like sampling a particular pmf from a distribution. The probability of picking a dice, which results in a pmf, comes from the Dirichlet distribution &lt;script type=&quot;math/tex&quot;&gt;Dir(\boldsymbol\alpha)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;\mathbf{q}=[q_1,q_2,...,q_k]&lt;/script&gt; be a pmf (or a point in simplex &lt;script type=&quot;math/tex&quot;&gt;\in&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\mathbf{R}^{k}&lt;/script&gt;), where &lt;script type=&quot;math/tex&quot;&gt;\sum\limits_{j=1}^{k}q_j=1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{q} \sim Dir(\boldsymbol\alpha)&lt;/script&gt;. Here &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol\alpha&lt;/script&gt; is dirichlet parameter &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol\alpha=[\alpha_1,\alpha_2,...,\alpha_k]&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\alpha_0=\sum\limits_{j=1}^{k}\alpha_j&lt;/script&gt;. Then, the probability density of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{q}&lt;/script&gt; is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(q,\boldsymbol\alpha)=p(q|\boldsymbol\alpha)=\frac{\Gamma(\alpha_0)}{\prod\limits_{j=1}^{k}\Gamma(\alpha_j)}\prod\limits_{j=1}^{k}q_j^{\alpha_j-1}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Graphical Model:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose &lt;script type=&quot;math/tex&quot;&gt;\{x_i\}&lt;/script&gt; is a set of samples drawn from &lt;script type=&quot;math/tex&quot;&gt;i^{th}&lt;/script&gt; pmf where &lt;script type=&quot;math/tex&quot;&gt;i\in[1,L]&lt;/script&gt;. For eg. &lt;script type=&quot;math/tex&quot;&gt;\{x_i\}&lt;/script&gt; could be a sequence of outputs of a dice &lt;script type=&quot;math/tex&quot;&gt;\{1,2,1,3,4,3,6,..\}&lt;/script&gt;. Let &lt;script type=&quot;math/tex&quot;&gt;\mathbf{q}_1,\mathbf{q}_2,...,\mathbf{q}_L&lt;/script&gt; are &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; pmfs. Then:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
p(\{x_i\}|\boldsymbol\alpha)&amp;=\int p(\{x_i\},\mathbf{q}_i|\boldsymbol\alpha)d\mathbf{q}_i\\
&amp;=\int p(\{x_i\}|\mathbf{q}_i,\boldsymbol\alpha) p(\mathbf{q}_i|\boldsymbol\alpha)  d\mathbf{q}_i\\
&amp;=\int p(\{x_i\}|\mathbf{q}_i) p(\mathbf{q}_i|\boldsymbol\alpha)  d\mathbf{q}_i\\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;n_{ij}&lt;/script&gt; be the number of outcomes in &lt;script type=&quot;math/tex&quot;&gt;\{x_i\}&lt;/script&gt; sequence of samples that is equal to &lt;script type=&quot;math/tex&quot;&gt;j^{th}&lt;/script&gt; event where &lt;script type=&quot;math/tex&quot;&gt;j\in[1,k]&lt;/script&gt;, and let &lt;script type=&quot;math/tex&quot;&gt;n_i = \sum\limits_{j=1}^{k} n_{ij}&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\{x_i\}|\mathbf{q}_i)=\frac{n_i!}{\prod\limits_{j=1}^{k}n_{ij}!} \prod\limits_{j=1}^{k} q_{ij}^{n_{ij}}&lt;/script&gt;

&lt;p&gt;Therefore,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
p(\{x_i\}|\boldsymbol\alpha)&amp;=\int \frac{n_i!}{\prod\limits_{j=1}^{k}n_{ij}!} \prod\limits_{j=1}^{k} q_{ij}^{n_{ij}} \times \frac{\Gamma(\alpha_0)}{\prod\limits_{j=1}^{k}\Gamma(\alpha_j)}\prod\limits_{j=1}^{k}q_{ij}^{\alpha_j-1}  d\mathbf{q}_i\\
&amp;=  \frac{ \Gamma(\alpha_0) n_i! }{\prod\limits_{j=1}^{k}   \Gamma(\alpha_j) n_{ij}!}   \int \prod\limits_{j=1}^{k} q_{ij}^{n_{ij}+\alpha_j-1} d\mathbf{q}_i\\
&amp;=\frac{ \Gamma(\alpha_0) n_i! }{\Gamma (\sum\limits_{j=1}^{k} (n_{ij}+ \alpha_j))}  \prod\limits_{j=1}^{k} \frac{ \Gamma(n_{ij}+\alpha_j) }{\Gamma(\alpha_j) n_{ij}!}\\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;blockquote&gt;
  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\{x_i\}|\boldsymbol\alpha)=\frac{ \Gamma(\alpha_0) n_i! }{\Gamma (\sum\limits_{j=1}^{k} (n_{ij}+ \alpha_j))}  \prod\limits_{j=1}^{k} \frac{ \Gamma(n_{ij}+\alpha_j) }{\Gamma(\alpha_j) n_{ij}!}&lt;/script&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
  &lt;li&gt;Bela A. Frigyik, Amol Kapila, and Maya R. Gupta. “Introduction to the Dirichlet Distribution and Related
Processes”. &lt;a href=&quot;http://mayagupta.org/publications/FrigyikKapilaGuptaIntroToDirichlet.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 03 Jun 2016 19:01:00 -0500</pubDate>
        <link>http://localhost:4000/tutorials/2016/06/03/dirichlet-distribution.html</link>
        <guid isPermaLink="true">http://localhost:4000/tutorials/2016/06/03/dirichlet-distribution.html</guid>
        
        
        <category>Tutorials</category>
        
      </item>
    
      <item>
        <title> Blog: Bayesian Sets.</title>
        <description>&lt;p&gt;&lt;strong&gt;Bayesian Sets Graphical Model:&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/images/bayesian-sets.svg&quot; style=&quot;width: 35%; height: 35%&quot; /&gt;&lt;/div&gt;

&lt;p&gt;Bayesian sets are simple graphical models used to expand a set. For instance, suppose you are given a set of few words (or items) &lt;script type=&quot;math/tex&quot;&gt;S=\{cat, dog, lion, ...\}&lt;/script&gt; which we refer as “seeds” and we wish to expand this set to include all similar words from the given text corpus. Then, we can employ Bayesian sets which rank each item based on its importance of belonging to seed set.&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; be a data set of items, and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x} \in D&lt;/script&gt; be an item from this set. Assume the user provides a query set &lt;script type=&quot;math/tex&quot;&gt;D_c&lt;/script&gt; which is a small subset of &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;, bayesian sets rank each item   by &lt;script type=&quot;math/tex&quot;&gt;score(\mathbf{x})&lt;/script&gt;. This probability ratio  can be interpreted as the ratio of the joint probability of observing &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;D_c&lt;/script&gt; to the probability of independently observing &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;D_c&lt;/script&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;script type=&quot;math/tex; mode=display&quot;&gt;score(\mathbf{x})=\frac{p(\mathbf{x}\vert D_c)}{p(\mathbf{x})}= \frac{p(\mathbf{x}, D_c)}{p(\mathbf{x})p(D_c)}=\frac{\int p(\mathbf{x},\theta \vert D_c ) d\theta}{\int p(\mathbf{x},\theta) d\theta}=\frac{\int p(\mathbf{x} \vert \theta, D_c ) p(\theta \vert D_c) d\theta}{\int p(\mathbf{x} \vert \theta) p(\theta )d\theta}&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;Assume that the parameterized model is &lt;script type=&quot;math/tex&quot;&gt;p(\mathbf{x} \vert \theta)&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; are the parameters as shown in figure above. Here, we assume that &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; is represented by a binary feature vector and &lt;script type=&quot;math/tex&quot;&gt;\theta_j&lt;/script&gt; is the weight associated with feature &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;. Then,&lt;/p&gt;

&lt;p&gt;For each &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}_i&lt;/script&gt; (Note: &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; vector &lt;script type=&quot;math/tex&quot;&gt;j^{th}&lt;/script&gt; component is &lt;script type=&quot;math/tex&quot;&gt;x_{.j}&lt;/script&gt; and bold letters are vectors):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathbf{x}_i\vert \boldsymbol\theta)=\prod\limits_{j=1}^{J} \theta_{j}^{x_{ij}} (1-\theta_j)^{1-x_{ij}}&lt;/script&gt;

&lt;p&gt;The conjugate prior for the parameters of a Bernoulli distribution is the Beta distribution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
p(\boldsymbol\theta\vert \boldsymbol\alpha, \boldsymbol\beta) &amp;=\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}	 \theta_{j}^{\alpha_j-1} (1-\theta_j)^{\beta_j-1} \\
p(\mathbf{x} \vert \boldsymbol\alpha, \boldsymbol\beta) &amp;=\int p(\mathbf{x} \vert \theta, \boldsymbol\alpha, \boldsymbol\beta) p(\theta )d\theta=\int p(\mathbf{x} \vert \theta) p(\theta \vert \boldsymbol\alpha, \boldsymbol\beta )d\theta \\
&amp;=\int \prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}	 \theta_{j}^{\alpha_j-1+x_{.j}} (1-\theta_j)^{\beta_j -x_{.j} } d\theta\\
&amp;= \prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}	 \int_{0}^{1}  \theta_{j}^{\alpha_j-1+x_{.j}} (1-\theta_j)^{\beta_j -x_{.j} } d\theta_j \\
&amp;=\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)} \frac{\Gamma(\alpha_j+x_{.j})\Gamma(\beta_j+1-x_{.j})}{\Gamma(\alpha_j+\beta_j+1)}	\\
p(D_c \vert \boldsymbol\alpha, \boldsymbol\beta)&amp;=\int p(D_c, \theta) d\theta=\int p(D_c \vert \theta)p(\theta \vert \boldsymbol\alpha, \boldsymbol\beta) d\theta \\
&amp;=\int \Big(\prod\limits_{i=1}^{N} p(\mathbf{x}_i\vert \theta)\Big)p(\theta \vert \boldsymbol\alpha, \boldsymbol\beta) d\theta \\
&amp;=\int \Big(\prod\limits_{i=1}^{N}\Big(\prod\limits_{j=1}^{J} \theta_{j}^{x_{ij}} (1-\theta_j)^{1-x_{ij}}\Big)\Big)\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}	 \theta_{j}^{\alpha_j-1} (1-\theta_j)^{\beta_j-1}d\theta \\
&amp;=\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}	 \int_{0}^{1}\theta_{j}^{\alpha_j-1+\sum_{i=1}^{N}x_{ij}} (1-\theta_j)^{\beta_j -\sum_{i=1}^{N}x_{.j}+N+1 } d\theta_j \\
&amp;= \prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}   \frac{\Gamma(\widetilde\alpha_j)\Gamma(\widetilde\beta_j)}{\Gamma(\widetilde\alpha_j+\widetilde\beta_j)} \\
&amp;\text{where},  \widetilde\alpha_j= \alpha_j+\sum_{i=1}^{N}x_{ij} , \hspace{1em} \widetilde\beta_j=\beta_j+N-\sum_{i=1}^{N}x_{ij}\\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
p(\mathbf{x}\vert D_c,\mathbf{\alpha,\beta})&amp;= \int p(\mathbf{x} \vert \theta) p(\theta\vert D_c,\alpha,\beta)d\theta \\
&amp;=\int p(\mathbf{x} \vert \theta)\frac{p(D_c \vert \theta, \alpha,\beta)p(\theta \vert \alpha,\beta)}{p(D_c\vert \alpha,\beta)} d\theta  \\
&amp;= \frac{1}{p(D_c\vert \alpha,\beta)} \int \Big(\prod\limits_{j=1}^{J} \theta_{j}^{x_{.j}} (1-\theta_j)^{1-x_{.j}}\Big) \times \Big(\prod\limits_{i=1}^{N}p(x_i\vert \theta)\Big) \times \\
&amp;\Big(\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}	 \theta_{j}^{\alpha_j-1} (1-\theta_j)^{\beta_j-1}\Big) d\theta\\
&amp;=\prod\limits_{j=1}^{J} \frac{\Gamma(\widetilde\alpha_j+\widetilde\beta_j)}{\Gamma(\widetilde\alpha_j)\Gamma(\widetilde\beta_j)} \int \theta_j^{x_{.j}+\sum_{i=1}^{N}x_{ij}+\alpha_j-1}(1-\theta_j)^{x.j+N-\sum_{i=1}^{N}x_{ij}+\beta_j}d\theta \\
&amp;=\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j+N)}{\Gamma(\widetilde\alpha_j)\Gamma(\widetilde\beta_j)}\frac{\Gamma(\widetilde\alpha_j+x.j)\Gamma(\widetilde\beta_j+1-x_{.j})}{\Gamma(\alpha_j+\beta_j+N+1)} \\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
score(\mathbf{x})&amp;=\frac{p(\mathbf{x}\vert D_c,\mathbf{\alpha,\beta})}{p(\mathbf{x}\vert \mathbf{\alpha,\beta})}= \\
&amp;=\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j+N)}{\Gamma(\widetilde\alpha_j)\Gamma(\widetilde\beta_j)}\frac{\Gamma(\widetilde\alpha_j+x.j)\Gamma(\widetilde\beta_j+1-x_{.j})}{\Gamma(\alpha_j+\beta_j+N+1)} \times \\
&amp; \frac{\Gamma(\alpha_j)\Gamma(\beta_j)}{\Gamma(\alpha_j+\beta_j)} \frac{\Gamma(\alpha_j+\beta_j+1)}{\Gamma(\alpha_j+x_{.j})\Gamma(\beta_j+1-x_{.j})} \\
&amp;=\prod\limits_{j=1}^{J} \frac{\alpha_j+\beta_j}{\alpha_j+\beta_j+N} \Big(\frac{\widetilde\alpha_j}{\alpha_j}\Big)^{x_{.j}} \Big(\frac{\widetilde\beta_j}{\beta_j}\Big)^{1-x_{.j}}
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;The log of the score is linear in features of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\log score(\mathbf{x})=c+\sum_j q_j x{.j}&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thus, bayesian sets essentially performs &lt;strong&gt;feature selection&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;For models where &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; is parametrized using exponential families, we have a similar expression but may or may not be linear in features of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exponential Families:&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
p(\mathbf{x}\vert \theta)&amp;=f(\mathbf{x})g(\theta)e^{\theta^{T} u(\mathbf{x})} \\
p(\theta \vert \eta,\nu)&amp;= h(\eta,\nu)g(\theta)^{\eta}e^{\theta^{T}\nu} \\
h(\eta,\nu)&amp;=\frac{1}{\int g(\theta)^{\eta}e^{\theta^T\nu}d\theta}\\
p(\mathbf{x} \vert \eta,\nu)&amp;=\int f(\mathbf{x})g(\theta)e^{\theta^T u(\mathbf{x})}h(\eta,\nu)g(\theta)^{\eta}e^{\theta^T\nu}d\theta\\
&amp;=\frac{f(\mathbf{x}) h(\eta,\nu)}{h(\eta+1,u(\mathbf{x})+\nu)} \\
p(D_c \vert \eta,\nu)&amp;=\int \Big(\prod\limits_{i=1}^{N} f(\mathbf{x_i})g(\theta)e^{\theta^{T} u(\mathbf{x_i})}\Big) h(\eta,\nu)g(\theta)^{\eta}e^{\theta^T\nu}d\theta\\
&amp;=\frac{h(\eta,\nu)\Big(\prod\limits_{i=1}^{N} f(\mathbf{x_i})\Big)}{h(\eta+N,\sum\limits_{i=1}^{N}u(\mathbf{x})+\nu)}\\
p(\mathbf{x} \vert D_c, \eta, \nu)&amp;= \int f(\mathbf{x})g(\theta)e^{\theta^T u(\mathbf{x})}\Big(\prod\limits_{i=1}^{N} f(\mathbf{x_i})g(\theta)e^{\theta^{T} u(\mathbf{x_i})}\Big)  \times \\
&amp; h(\eta,\nu)g(\theta)^{\eta}e^{\theta^T\nu}d\theta\\
&amp;=\frac{h(\eta,\nu)f(x)\Big(\prod\limits_{i=1}^{N} f(\mathbf{x_i})\Big)}{p(D_c \vert \eta,\nu) h{(\eta+N+1,\nu+\sum_{i=1}^{N}u(\mathbf{x_i})+u(\mathbf{x})})}\\
score(\mathbf{x})&amp;=\frac{ h(\eta+1,\nu+u(\mathbf{x}))  h(\eta+N,\nu+\sum\limits_{i=1}^{N}u(\mathbf{x}_i))}{h(\eta,\nu) h{(\eta+N+1,\nu+\sum_{i=1}^{N}u(\mathbf{x_i})+u(\mathbf{x})})}
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Ghahramani, Zoubin, and Katherine A. Heller. “Bayesian sets.” NIPS. Vol. 2. 2005. &lt;a href=&quot;https://papers.nips.cc/paper/2817-bayesian-sets.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Verma, Saurabh, and Estevam R. Hruschka Jr. “Coupled bayesian sets algorithm for semi-supervised learning and information extraction.” Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer Berlin Heidelberg, 2012. &lt;a href=&quot;http://rtw.ml.cmu.edu/papers/ecml12-verma.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 01 Jun 2016 19:01:00 -0500</pubDate>
        <link>http://localhost:4000/tutorials/2016/06/01/bayesians-sets.html</link>
        <guid isPermaLink="true">http://localhost:4000/tutorials/2016/06/01/bayesians-sets.html</guid>
        
        
        <category>Tutorials</category>
        
      </item>
    
      <item>
        <title> Blog: Why Regularized Auto-Encoders learn Sparse Representation?</title>
        <description>&lt;p&gt;This paper has some great insights to offer in design of autoencoders. As title suggests, it addresses “whether regularized helps in learning sparse representation of the data theoretically?”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Here is the setup:&lt;/strong&gt; we have an input &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x} \in \mathbf{R}^{d}&lt;/script&gt; which is mapped to latent space &lt;script type=&quot;math/tex&quot;&gt;\mathbf{h=s(Wx+b)}&lt;/script&gt; via autoencoder where &lt;script type=&quot;math/tex&quot;&gt;\mathbf{s}&lt;/script&gt; is encoder activation function, &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W} \in \mathbf{R}^{m\times n}&lt;/script&gt; is the weight matrix, and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{b} \in \mathbf{R^m}&lt;/script&gt; is the encoder bias and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{h} \in \mathbf{R^m}&lt;/script&gt; is hidden representation or outputs of hidden units.&lt;/p&gt;

&lt;p&gt;For analysis, paper assumes that decoder is linear i.e. &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W^{T}h}&lt;/script&gt; decodes back the encoded hidden representation and loss is squared loss function. Therefore, for learning &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W,b}&lt;/script&gt; parameters of autoencoder; objective function is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
J_{AE} &amp;=\mathbb{E_x}[(\mathbf{x-W^Th})^2] \\
&amp;= \mathbb{E_x}[(\mathbf{x-W^Ts(Wx+b)})^2]\\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Now we are interested in sparsity of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{h}&lt;/script&gt;, hidden representation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Besides above, paper makes two more assumptions.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Assumption 1&lt;/strong&gt;: Data is drawn from a distribution &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x} \sim  X&lt;/script&gt; for which &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E_x}[\mathbf{x}]=0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E_x}[\mathbf{xx^T]=I}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;\mathbf{I}&lt;/script&gt; is identity matrix.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Basically, it assumes that data is whitened which is reasonable for some cases. There is one more assumption from analysis point of view which is needed for the derivation of theorems, we will see that later.&lt;/p&gt;

&lt;p&gt;Finally, for a give data sample &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt;, each hidden unit &lt;script type=&quot;math/tex&quot;&gt;h_j&lt;/script&gt; gets activated if pre-activation unit &lt;script type=&quot;math/tex&quot;&gt;a_j&lt;/script&gt; is greater than &lt;script type=&quot;math/tex&quot;&gt;\delta&lt;/script&gt; threshold:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_j=W_j\mathbf{x}+b_j&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_j=s(a_j)&lt;/script&gt;

&lt;p&gt;So, the sparsity of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{h}&lt;/script&gt; depends upon the value of pre-activation units (whose value further depend upon the input data samples). If the expected value of pre-activation unit is less than threshold over data distribution then the corresponding hidden unit expected output is  zero.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The idea is to somehow show that regularization encourages the expected value of pre-activation unit &lt;script type=&quot;math/tex&quot;&gt;a_j, \forall j&lt;/script&gt;  to reduce on average in autoencoders. This indirectly induces sparsity in &lt;script type=&quot;math/tex&quot;&gt;\mathbf{h}&lt;/script&gt;, hidden representation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;This should hint us that:&lt;/strong&gt; monotonically increasing activation functions would be preferable because we want to decrease hidden unit value as the value of pre-activation unit decreases. On the top of that, if we consider monotonically increasing activation functions with negative saturation at &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;, i.e. &lt;script type=&quot;math/tex&quot;&gt;\lim_{a\to-\infty} s(a)=0&lt;/script&gt;  then we can make sure that lower average pre-activation value implies higher sparsity. For example, consider &lt;script type=&quot;math/tex&quot;&gt;tanh&lt;/script&gt; function whose range is &lt;script type=&quot;math/tex&quot;&gt;(-1,1)&lt;/script&gt;. If we reduce &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; below zero, hidden unit output will move away from &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Let’s focus on analysis now. Assume we want to compute &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W,b}&lt;/script&gt; using gradient descent. Then at &lt;script type=&quot;math/tex&quot;&gt;t^{th}&lt;/script&gt; iteration of updating, we will have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_j^{t}=W_j^{t}\mathbf{x}+b_j^{t}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;a_j^{t}, W_j^{t}, b_j^{t}&lt;/script&gt; are values in &lt;script type=&quot;math/tex&quot;&gt;t^{th}&lt;/script&gt; iteration.&lt;/p&gt;

&lt;p&gt;Let regularized objective function of autoencoder is expressed as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J_{RAE}=J_{AE}+\lambda R(\mathbf{W,b})&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;R(\mathbf{W,b})&lt;/script&gt; is the regularization term and &lt;script type=&quot;math/tex&quot;&gt;\lambda&gt;0&lt;/script&gt;. Simplifying the main theorem of the paper over here:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;: For each iteration: &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\mathbb{E_x}[a_j^{t+1}]&lt;\mathbb{E_x}[a_j^{t}] %]]&gt;&lt;/script&gt;  holds for &lt;script type=&quot;math/tex&quot;&gt;\forall j&lt;/script&gt; with probability &lt;script type=&quot;math/tex&quot;&gt;(1-\frac{\sigma^2}{\epsilon^2})&lt;/script&gt;, if following conditions are met: &lt;script type=&quot;math/tex&quot;&gt;\hspace{5em}\lambda \frac{\partial R}{\partial b_j} &gt; 2\epsilon \sqrt{n}  \|W_j\|&lt;/script&gt; and encoding activation function, &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;, first derivative is bounded in &lt;script type=&quot;math/tex&quot;&gt;[0,1]&lt;/script&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The above theorem shows that  updating &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W,b}&lt;/script&gt; along the negative gradient of &lt;script type=&quot;math/tex&quot;&gt;J_{RAE}&lt;/script&gt; results in &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\mathbb{E_x}[a_j^{t+1}]&lt;\mathbb{E_x}[a_j^{t}] %]]&gt;&lt;/script&gt; with high probability if particular regularization condition is met (plus activation function condition). This means certain regularization terms explicitly encourages sparsity in hidden representation.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Corollary 1&lt;/strong&gt;: Theorem 1 holds for two special cases: &lt;br /&gt;
1)  activation function &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; is non-decreasing and regularization term has form &lt;script type=&quot;math/tex&quot;&gt;R=\sum\limits_{j=1}^{m}f(\mathbb{E_x}[h_j])&lt;/script&gt; for some monotonically increasing function &lt;script type=&quot;math/tex&quot;&gt;f.&lt;/script&gt;&lt;br /&gt;
2)  activation function &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; is convex plus non-decreasing and regularization term has form &lt;script type=&quot;math/tex&quot;&gt;R=\mathbb{E_x}[\sum\limits_{j=1}^{m}(\frac{\partial h_j}{\partial a_j})^q \|W_j\|^{p}_2]&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; is natural and &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; whole number.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;We are ready to make some remarks about in practice activation functions!&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;ReLu&lt;/strong&gt;: It satisfies both corollaries. Hence can be used with any regularized of first form but not the second form (because second derivative does not exist). The advantage of ReLU is
that it enforces hard zeros in the learned representations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Softplus&lt;/strong&gt;: It satisfies both corollaries and encourages sparsity for both suggested regularization form. But does not produce hard zeros.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Sigmoid&lt;/strong&gt;: Satisfies only first corollary.  Hence sigmoid is not guaranteed to lead to sparsity when used with second regularizations form.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Maxout and Tanh&lt;/strong&gt;: Do not satisfies negative saturation property at &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; and hence may or may not lead to sparsity.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Final remarks about about in practice regularized autoencoders!&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;De-noising Autoencoder (DAE) has regularization of second form.&lt;/li&gt;
  &lt;li&gt;Contractive Auto-Encoder (CAE) including higher order has  regularization of second form.&lt;/li&gt;
  &lt;li&gt;Marginalized De-noising Auto-Encoder (mDAE) has  regularization of second form.&lt;/li&gt;
  &lt;li&gt;Sparse Auto-Encoder (SAE) has regularization of first form.&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
  &lt;p&gt;Thus, all above autoencoders encourages sparsity theoretically, if chosen with appropriate activation function.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Arpit, D., Zhou, Y., Ngo, H., &amp;amp; Govindaraju, V. (2015). Why Regularized Auto-Encoders learn Sparse Representation? ICML2016. &lt;a href=&quot;http://arxiv.org/pdf/1505.05561.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Tue, 31 May 2016 17:37:00 -0500</pubDate>
        <link>http://localhost:4000/paperlist/2016/05/31/why-autoencoder-sparse.html</link>
        <guid isPermaLink="true">http://localhost:4000/paperlist/2016/05/31/why-autoencoder-sparse.html</guid>
        
        
        <category>PaperList</category>
        
      </item>
    
      <item>
        <title>Blog: Learning Convolutional Networks for Graphs</title>
        <description>&lt;p&gt;For past couple of months, I have been wondering about how convolutional networks can be applied to  graphs. As we know, convolutional networks have became the state of the art in image classification and also in natural language processing. It is easy to see how convolutional networks  exploits the locality and translation invariance properties in an image. People have now also realized on how to exploit those properties in NLP using convolutional networks efficiently. It is time to look at other, more general, domains such as  &lt;em&gt;graphs&lt;/em&gt; where notion of locality or receptive field needs to be defined. At this point, we can start thinking about graph neighborhood concepts as it is going to play a major role in connecting with receptive fields of convolutional networks.&lt;/p&gt;

&lt;p&gt;There are two problem formulation exist in literature:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Given a set of graphs &lt;script type=&quot;math/tex&quot;&gt;S_G=\{G_1,G_2,...,G_n\}&lt;/script&gt; and labels &lt;script type=&quot;math/tex&quot;&gt;Y=\{y_1,y_2,...,y_n\}\in \mathbf{R}&lt;/script&gt;, learn a function &lt;script type=&quot;math/tex&quot;&gt;f:G\rightarrow\mathbf{R}&lt;/script&gt; such that it maps graphs to appropriate labels using convolutional networks.&lt;/li&gt;
    &lt;li&gt;Generalization of convolutional network to a single graph &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; where each input example is a vertex.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;The first problem is somewhat becomes usual machine learning problem, if we know how to construct/extract features from a graph and then we can simply train any machine learning algorithm. Second problem, is different in the sense that each node itself is a data point in a given graph, so it is not apparent what could be a potential feature vector here. Personally, I am more interested in second the one.&lt;/p&gt;

&lt;p&gt;Without diving into previous works (there are many), I’ll layout the findings of:  &lt;em&gt;Learning Convolutional Neural Networks for Graphs&lt;/em&gt; ICML2016 paper which deal with the first problem formulation. The idea is to bring an order among nodes in a graph. So authors introduce the notion of graph labeling which decide the rank of nodes. Following steps are involved in their overall approach:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;First compute a graph labeling to define rank on the nodes. For example, if &lt;script type=&quot;math/tex&quot;&gt;l(u) &gt; l(v)&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;r(u)&gt;r(v)&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;l(u)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;r(u)&lt;/script&gt; is the label &amp;amp; rank of node &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; respectively. This labeling can be done based on degree of node, page-rank etc.&lt;/li&gt;
    &lt;li&gt;Next sort the vertices/nodes of the input graph with respect to a given graph labeling. Then the resulting node sequence, say &lt;script type=&quot;math/tex&quot;&gt;NS&lt;/script&gt; which is 1-dimensional, is traversed using a given stride &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;.&lt;/li&gt;
    &lt;li&gt;For each visited node (through stride &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;) in &lt;script type=&quot;math/tex&quot;&gt;NS&lt;/script&gt;, construct a receptive field, until exactly &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; receptive fields have been created. (Wait. What is receptive field here ?)&lt;/li&gt;
    &lt;li&gt;Receptive field is constructed based on the neighborhood of a node. This neighborhood can be defined based on adjacent nodes or shortest distance between nodes. Given   a node &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; and the size of the receptive field &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, the procedure performs a breadth-first search to find exactly &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; neighbor nodes. Special care is taken when there are less  or more than &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; (in case of tie) neighbor nodes. We can call this neighborhood of a node as sub-graph neighborhood.&lt;/li&gt;
    &lt;li&gt;On each sub-graph neighborhood, final and crucial step is performed called graph normalization and canonicalization.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;To understand what graph canonicalization is, you need to understand what is graph isomorphism. Loosely speaking, if two graphs say &lt;script type=&quot;math/tex&quot;&gt;G_1&lt;/script&gt; &amp;amp; &lt;script type=&quot;math/tex&quot;&gt;G_2&lt;/script&gt; have different vertex labels and edge labels but graph &lt;script type=&quot;math/tex&quot;&gt;G_1&lt;/script&gt; can be re-labeled such that adjacency matrix becomes equal to graph &lt;script type=&quot;math/tex&quot;&gt;G_2&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;G_1&lt;/script&gt; is isomorphic to &lt;script type=&quot;math/tex&quot;&gt;G_2&lt;/script&gt;. In other words they are the same (in structure), its just their labellings are different. So, how do we know if two graphs are isomorphic or not. If somehow, we can represent each graph in some form, say a string of binary numbers, then we can just check if the binary representation of two graphs is same or not. This will tell us whether graphs are isomorphic or not. Of-course, this representation needs to hold some properties for such type of checking. This representation through which we can determine isomorphism is called canonical representation of a graph or &lt;script type=&quot;math/tex&quot;&gt;canon(G)&lt;/script&gt;.  Basically, in canonical representation you can compare the two graphs just as you can compare the two images of cat. Canonicalization is a NP-hard problem, so necessary optimization is required.
Finally:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Each canonical representation of sub-graph neighborhood  becomes the receptive field  and   combined with the normal convolutional networks. The vertex (and edge) attributes becomes the channel (like three RGB channels in an image).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So, magic lies in canonicalization of a graph for which author resorts to something called &lt;em&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Nauty&lt;/code&gt;&lt;/em&gt; software, which performs canonicalization. I don’t know the details of the canonicalization algorithm but it does take degree of a node into account.&lt;/p&gt;

&lt;p&gt;This is the high-level approach authors adopt and experimentally it outperform the classification accuracy obtained by graph kernel algorithms for many datasets (eg. protein structure and chemical compounds related).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Niepert, Mathias, Mohamed Ahmed, and Konstantin Kutzkov. “Learning Convolutional Neural Networks for Graphs.” ICML 2016. &lt;a href=&quot;https://arxiv.org/pdf/1605.05273v2.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Henaff, Mikael, Joan Bruna, and Yann LeCun. “Deep convolutional networks on graph-structured data.” arXiv preprint arXiv:1506.05163 (2015).&lt;a href=&quot;http://arxiv.org/pdf/1506.05163v1.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Bruna, Joan, et al. “Spectral networks and locally connected networks on graphs.” arXiv preprint arXiv:1312.6203 (2013). &lt;a href=&quot;https://arxiv.org/pdf/1312.6203.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Coates, Adam, and Andrew Y. Ng. “Selecting receptive fields in deep networks.” Advances in Neural Information Processing Systems. 2011. &lt;a href=&quot;http://robotics.stanford.edu/~ang/papers/nips11-SelectingReceptiveFields.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Yanardag, Pinar, and S. V. N. Vishwanathan. “Deep graph kernels.” Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2015.&lt;a href=&quot;http://web.ics.purdue.edu/~ypinar/kdd/deep_graph_kernels.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 29 May 2016 17:37:00 -0500</pubDate>
        <link>http://localhost:4000/paperlist/2016/05/29/convgraph.html</link>
        <guid isPermaLink="true">http://localhost:4000/paperlist/2016/05/29/convgraph.html</guid>
        
        
        <category>PaperList</category>
        
      </item>
    
      <item>
        <title>Reading List: ICML Papers 2016 </title>
        <description>&lt;p&gt;To make a habit of reading more papers, I have decided to write comments, may be some quick or sometime detail, on  the papers which I find interesting and related to my research area. Hopefully, this will come handy in solving my own research problems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Starting with ICML 2016 Papers:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Revisiting Semi-Supervised Learning with Graph Embeddings ICML 2016&lt;/li&gt;
  &lt;li&gt;Why Regularized Auto-Encoders learn  Sparse Representation? ICML 2016&lt;/li&gt;
  &lt;li&gt;On the Consistency of Feature Selection With Lasso for Non-linear Targets. ICML 2016&lt;/li&gt;
  &lt;li&gt;Additive Approximations in High Dimensional Nonparametric Regression via the SALSA. ICML 2016&lt;/li&gt;
  &lt;li&gt;The Variational Nystrom method for large-scale spectral problems. ICML 2016&lt;/li&gt;
  &lt;li&gt;A Kernelized Stein Discrepancy for Goodness-of-fit Tests and Model Evaluation. ICML 2016 (Possibly a new way to measure difference in probability distribution other than most common – KL divergence )&lt;/li&gt;
  &lt;li&gt;Low-Rank Matrix Approximation with Stability. ICML 2016&lt;/li&gt;
  &lt;li&gt;Unsupervised Deep Embedding for Clustering Analysis. ICML 2016&lt;/li&gt;
  &lt;li&gt;Online Low-Rank Subspace Clustering by Explicit Basis Modeling. ICML 2016&lt;/li&gt;
  &lt;li&gt;Community Recovery in Graphs with Locality. ICML 2016&lt;/li&gt;
  &lt;li&gt;Analysis of Deep Neural Networks with Extended Data Jacobian Matrix. ICML 2016&lt;/li&gt;
  &lt;li&gt;Stochastic Variance Reduced Optimization for Nonconvex Sparse Learning. ICML 2016&lt;/li&gt;
  &lt;li&gt;Compressive Spectral Clustering ICML. 2016&lt;/li&gt;
  &lt;li&gt;Variance-Reduced and Projection-Free Stochastic Optimization. ICML 2016&lt;/li&gt;
  &lt;li&gt;Learning Convolutional Neural Networks for Graphs. ICML 2016&lt;/li&gt;
  &lt;li&gt;Discrete Deep Feature Extraction: A Theory and New Architectures.  ICML 2016&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 28 May 2016 17:37:00 -0500</pubDate>
        <link>http://localhost:4000/paperlist/2016/05/28/ReadingListICML16.html</link>
        <guid isPermaLink="true">http://localhost:4000/paperlist/2016/05/28/ReadingListICML16.html</guid>
        
        
        <category>PaperList</category>
        
      </item>
    
  </channel>
</rss>
