<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog: Machine Learning Equations by Saurabh Verma</title>
    <description>Blog: Machine Learning Equations by Saurabh Verma</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 03 Jan 2017 18:18:10 -0600</pubDate>
    <lastBuildDate>Tue, 03 Jan 2017 18:18:10 -0600</lastBuildDate>
    <generator>Jekyll v3.3.1</generator>
    
      <item>
        <title> Blog: Dirichlet Distributions.</title>
        <description>&lt;p&gt;Dirichlet Distribution is &lt;em&gt;a distribution over distributions&lt;/em&gt;. More specifically, it is a distribution over &lt;em&gt;pmfs (probability mass functions)&lt;/em&gt;. You can imagine, as if there is a bag of &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; dices, and each dice has a corresponding pmf (related to six possible outcomes). Now picking a dice is like sampling a particular pmf from a distribution. The probability of picking a dice, which results in a pmf, comes from the Dirichlet distribution &lt;script type=&quot;math/tex&quot;&gt;Dir(\boldsymbol\alpha)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;\mathbf{q}=[q_1,q_2,...,q_k]&lt;/script&gt; be a pmf (or a point in simplex &lt;script type=&quot;math/tex&quot;&gt;\in&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\mathbf{R}^{k}&lt;/script&gt;), where &lt;script type=&quot;math/tex&quot;&gt;\sum\limits_{j=1}^{k}q_j=1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{q} \sim Dir(\boldsymbol\alpha)&lt;/script&gt;. Here &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol\alpha&lt;/script&gt; is dirichlet parameter &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol\alpha=[\alpha_1,\alpha_2,...,\alpha_k]&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\alpha_0=\sum\limits_{j=1}^{k}\alpha_j&lt;/script&gt;. Then, the probability density of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{q}&lt;/script&gt; is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(q,\boldsymbol\alpha)=p(q|\boldsymbol\alpha)=\frac{\Gamma(\alpha_0)}{\prod\limits_{j=1}^{k}\Gamma(\alpha_j)}\prod\limits_{j=1}^{k}q_j^{\alpha_j-1}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Graphical Model:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose &lt;script type=&quot;math/tex&quot;&gt;\{x_i\}&lt;/script&gt; is a set of samples drawn from &lt;script type=&quot;math/tex&quot;&gt;i^{th}&lt;/script&gt; pmf where &lt;script type=&quot;math/tex&quot;&gt;i\in[1,L]&lt;/script&gt;. For eg. &lt;script type=&quot;math/tex&quot;&gt;\{x_i\}&lt;/script&gt; could be a sequence of outputs of a dice &lt;script type=&quot;math/tex&quot;&gt;\{1,2,1,3,4,3,6,..\}&lt;/script&gt;. Let &lt;script type=&quot;math/tex&quot;&gt;\mathbf{q}_1,\mathbf{q}_2,...,\mathbf{q}_L&lt;/script&gt; are &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; pmfs. Then:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
p(\{x_i\}|\boldsymbol\alpha)&amp;=\int p(\{x_i\},\mathbf{q}_i|\boldsymbol\alpha)d\mathbf{q}_i\\
&amp;=\int p(\{x_i\}|\mathbf{q}_i,\boldsymbol\alpha) p(\mathbf{q}_i|\boldsymbol\alpha)  d\mathbf{q}_i\\
&amp;=\int p(\{x_i\}|\mathbf{q}_i) p(\mathbf{q}_i|\boldsymbol\alpha)  d\mathbf{q}_i\\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;n_{ij}&lt;/script&gt; be the number of outcomes in &lt;script type=&quot;math/tex&quot;&gt;\{x_i\}&lt;/script&gt; sequence of samples that is equal to &lt;script type=&quot;math/tex&quot;&gt;j^{th}&lt;/script&gt; event where &lt;script type=&quot;math/tex&quot;&gt;j\in[1,k]&lt;/script&gt;, and let &lt;script type=&quot;math/tex&quot;&gt;n_i = \sum\limits_{j=1}^{k} n_{ij}&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\{x_i\}|\mathbf{q}_i)=\frac{n_i!}{\prod\limits_{j=1}^{k}n_{ij}!} \prod\limits_{j=1}^{k} q_{ij}^{n_{ij}}&lt;/script&gt;

&lt;p&gt;Therefore,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
p(\{x_i\}|\boldsymbol\alpha)&amp;=\int \frac{n_i!}{\prod\limits_{j=1}^{k}n_{ij}!} \prod\limits_{j=1}^{k} q_{ij}^{n_{ij}} \times \frac{\Gamma(\alpha_0)}{\prod\limits_{j=1}^{k}\Gamma(\alpha_j)}\prod\limits_{j=1}^{k}q_{ij}^{\alpha_j-1}  d\mathbf{q}_i\\
&amp;=  \frac{ \Gamma(\alpha_0) n_i! }{\prod\limits_{j=1}^{k}   \Gamma(\alpha_j) n_{ij}!}   \int \prod\limits_{j=1}^{k} q_{ij}^{n_{ij}+\alpha_j-1} d\mathbf{q}_i\\
&amp;=\frac{ \Gamma(\alpha_0) n_i! }{\Gamma (\sum\limits_{j=1}^{k} (n_{ij}+ \alpha_j))}  \prod\limits_{j=1}^{k} \frac{ \Gamma(n_{ij}+\alpha_j) }{\Gamma(\alpha_j) n_{ij}!}\\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;blockquote&gt;
  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\{x_i\}|\boldsymbol\alpha)=\frac{ \Gamma(\alpha_0) n_i! }{\Gamma (\sum\limits_{j=1}^{k} (n_{ij}+ \alpha_j))}  \prod\limits_{j=1}^{k} \frac{ \Gamma(n_{ij}+\alpha_j) }{\Gamma(\alpha_j) n_{ij}!}&lt;/script&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
  &lt;li&gt;Bela A. Frigyik, Amol Kapila, and Maya R. Gupta. “Introduction to the Dirichlet Distribution and Related
Processes”. &lt;a href=&quot;http://mayagupta.org/publications/FrigyikKapilaGuptaIntroToDirichlet.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Fri, 03 Jun 2016 19:01:00 -0500</pubDate>
        <link>http://localhost:4000/tutorials/2016/06/03/dirichlet-distribution.html</link>
        <guid isPermaLink="true">http://localhost:4000/tutorials/2016/06/03/dirichlet-distribution.html</guid>
        
        
        <category>Tutorials</category>
        
      </item>
    
      <item>
        <title> Blog: Bayesian Sets.</title>
        <description>&lt;p&gt;&lt;strong&gt;Bayesian Sets Graphical Model:&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/images/bayesian-sets.svg&quot; style=&quot;width: 35%; height: 35%&quot; /&gt;&lt;/div&gt;

&lt;p&gt;Bayesian sets are simple graphical models used to expand a set. For instance, suppose you are given a set of few words (or items) &lt;script type=&quot;math/tex&quot;&gt;S=\{cat, dog, lion, ...\}&lt;/script&gt; which we refer as “seeds” and we wish to expand this set to include all similiar words from the given text corpus. Then, we can employ Bayesian sets which rank each item based on its importance of belonging to seed set.&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; be a data set of items, and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x} \in D&lt;/script&gt; be an item from this set. Assume the user provides a query set &lt;script type=&quot;math/tex&quot;&gt;D_c&lt;/script&gt; which is a small subset of &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;, bayesian sets rank each item   by &lt;script type=&quot;math/tex&quot;&gt;score(\mathbf{x})&lt;/script&gt;. This probability ratio  can be interpreted as the ratio of the joint probability of observing &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;D_c&lt;/script&gt; to the probability of independently observing &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;D_c&lt;/script&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;script type=&quot;math/tex; mode=display&quot;&gt;score(\mathbf{x})=\frac{p(\mathbf{x}\vert D_c)}{p(\mathbf{x})}= \frac{p(\mathbf{x}, D_c)}{p(\mathbf{x})p(D_c)}=\frac{\int p(\mathbf{x},\theta \vert D_c ) d\theta}{\int p(\mathbf{x},\theta) d\theta}=\frac{\int p(\mathbf{x} \vert \theta, D_c ) p(\theta \vert D_c) d\theta}{\int p(\mathbf{x} \vert \theta) p(\theta )d\theta}&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;Assume that the parameterized model is &lt;script type=&quot;math/tex&quot;&gt;p(\mathbf{x} \vert \theta)&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; are the parameters as shown in figure above. Here, we assume that &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; is represented by a binary feature vector and &lt;script type=&quot;math/tex&quot;&gt;\theta_j&lt;/script&gt; is the weight associated with feature &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;. Then,&lt;/p&gt;

&lt;p&gt;For each &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}_i&lt;/script&gt; (Note: &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; vector &lt;script type=&quot;math/tex&quot;&gt;j^{th}&lt;/script&gt; component is &lt;script type=&quot;math/tex&quot;&gt;x_{.j}&lt;/script&gt; and bold letters are vectors):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\mathbf{x}_i\vert \boldsymbol\theta)=\prod\limits_{j=1}^{J} \theta_{j}^{x_{ij}} (1-\theta_j)^{1-x_{ij}}&lt;/script&gt;

&lt;p&gt;The conjugate prior for the parameters of a Bernoulli distribution is the Beta distribution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
p(\boldsymbol\theta\vert \boldsymbol\alpha, \boldsymbol\beta) &amp;=\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}	 \theta_{j}^{\alpha_j-1} (1-\theta_j)^{\beta_j-1} \\
p(\mathbf{x} \vert \boldsymbol\alpha, \boldsymbol\beta) &amp;=\int p(\mathbf{x} \vert \theta, \boldsymbol\alpha, \boldsymbol\beta) p(\theta )d\theta=\int p(\mathbf{x} \vert \theta) p(\theta \vert \boldsymbol\alpha, \boldsymbol\beta )d\theta \\
&amp;=\int \prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}	 \theta_{j}^{\alpha_j-1+x_{.j}} (1-\theta_j)^{\beta_j -x_{.j} } d\theta\\
&amp;= \prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}	 \int_{0}^{1}  \theta_{j}^{\alpha_j-1+x_{.j}} (1-\theta_j)^{\beta_j -x_{.j} } d\theta_j \\
&amp;=\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)} \frac{\Gamma(\alpha_j+x_{.j})\Gamma(\beta_j+1-x_{.j})}{\Gamma(\alpha_j+\beta_j+1)}	\\
p(D_c \vert \boldsymbol\alpha, \boldsymbol\beta)&amp;=\int p(D_c, \theta) d\theta=\int p(D_c \vert \theta)p(\theta \vert \boldsymbol\alpha, \boldsymbol\beta) d\theta \\
&amp;=\int \Big(\prod\limits_{i=1}^{N} p(\mathbf{x}_i\vert \theta)\Big)p(\theta \vert \boldsymbol\alpha, \boldsymbol\beta) d\theta \\
&amp;=\int \Big(\prod\limits_{i=1}^{N}\Big(\prod\limits_{j=1}^{J} \theta_{j}^{x_{ij}} (1-\theta_j)^{1-x_{ij}}\Big)\Big)\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}	 \theta_{j}^{\alpha_j-1} (1-\theta_j)^{\beta_j-1}d\theta \\
&amp;=\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}	 \int_{0}^{1}\theta_{j}^{\alpha_j-1+\sum_{i=1}^{N}x_{ij}} (1-\theta_j)^{\beta_j -\sum_{i=1}^{N}x_{.j}+N+1 } d\theta_j \\
&amp;= \prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}   \frac{\Gamma(\widetilde\alpha_j)\Gamma(\widetilde\beta_j)}{\Gamma(\widetilde\alpha_j+\widetilde\beta_j)} \\
&amp;\text{where},  \widetilde\alpha_j= \alpha_j+\sum_{i=1}^{N}x_{ij} , \hspace{1em} \widetilde\beta_j=\beta_j+N-\sum_{i=1}^{N}x_{ij}\\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
p(\mathbf{x}\vert D_c,\mathbf{\alpha,\beta})&amp;= \int p(\mathbf{x} \vert \theta) p(\theta\vert D_c,\alpha,\beta)d\theta \\
&amp;=\int p(\mathbf{x} \vert \theta)\frac{p(D_c \vert \theta, \alpha,\beta)p(\theta \vert \alpha,\beta)}{p(D_c\vert \alpha,\beta)} d\theta  \\
&amp;= \frac{1}{p(D_c\vert \alpha,\beta)} \int \Big(\prod\limits_{j=1}^{J} \theta_{j}^{x_{.j}} (1-\theta_j)^{1-x_{.j}}\Big) \times \Big(\prod\limits_{i=1}^{N}p(x_i\vert \theta)\Big) \times \\
&amp;\Big(\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j)}{\Gamma(\alpha_j)\Gamma(\beta_j)}	 \theta_{j}^{\alpha_j-1} (1-\theta_j)^{\beta_j-1}\Big) d\theta\\
&amp;=\prod\limits_{j=1}^{J} \frac{\Gamma(\widetilde\alpha_j+\widetilde\beta_j)}{\Gamma(\widetilde\alpha_j)\Gamma(\widetilde\beta_j)} \int \theta_j^{x_{.j}+\sum_{i=1}^{N}x_{ij}+\alpha_j-1}(1-\theta_j)^{x.j+N-\sum_{i=1}^{N}x_{ij}+\beta_j}d\theta \\
&amp;=\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j+N)}{\Gamma(\widetilde\alpha_j)\Gamma(\widetilde\beta_j)}\frac{\Gamma(\widetilde\alpha_j+x.j)\Gamma(\widetilde\beta_j+1-x_{.j})}{\Gamma(\alpha_j+\beta_j+N+1)} \\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
score(\mathbf{x})&amp;=\frac{p(\mathbf{x}\vert D_c,\mathbf{\alpha,\beta})}{p(\mathbf{x}\vert \mathbf{\alpha,\beta})}= \\
&amp;=\prod\limits_{j=1}^{J} \frac{\Gamma(\alpha_j+\beta_j+N)}{\Gamma(\widetilde\alpha_j)\Gamma(\widetilde\beta_j)}\frac{\Gamma(\widetilde\alpha_j+x.j)\Gamma(\widetilde\beta_j+1-x_{.j})}{\Gamma(\alpha_j+\beta_j+N+1)} \times \\
&amp; \frac{\Gamma(\alpha_j)\Gamma(\beta_j)}{\Gamma(\alpha_j+\beta_j)} \frac{\Gamma(\alpha_j+\beta_j+1)}{\Gamma(\alpha_j+x_{.j})\Gamma(\beta_j+1-x_{.j})} \\
&amp;=\prod\limits_{j=1}^{J} \frac{\alpha_j+\beta_j}{\alpha_j+\beta_j+N} \Big(\frac{\widetilde\alpha_j}{\alpha_j}\Big)^{x_{.j}} \Big(\frac{\widetilde\beta_j}{\beta_j}\Big)^{1-x_{.j}}
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;The log of the score is linear in features of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\log score(\mathbf{x})=c+\sum_j q_j x{.j}&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thus, bayesian sets essentially performs &lt;strong&gt;feature selection&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;For models where &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; is parametrized using exponential families, we have a similar expression but may or may not be linear in features of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exponential Families:&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
p(\mathbf{x}\vert \theta)&amp;=f(\mathbf{x})g(\theta)e^{\theta^{T} u(\mathbf{x})} \\
p(\theta \vert \eta,\nu)&amp;= h(\eta,\nu)g(\theta)^{\eta}e^{\theta^{T}\nu} \\
h(\eta,\nu)&amp;=\frac{1}{\int g(\theta)^{\eta}e^{\theta^T\nu}d\theta}\\
p(\mathbf{x} \vert \eta,\nu)&amp;=\int f(\mathbf{x})g(\theta)e^{\theta^T u(\mathbf{x})}h(\eta,\nu)g(\theta)^{\eta}e^{\theta^T\nu}d\theta\\
&amp;=\frac{f(\mathbf{x}) h(\eta,\nu)}{h(\eta+1,u(\mathbf{x})+\nu)} \\
p(D_c \vert \eta,\nu)&amp;=\int \Big(\prod\limits_{i=1}^{N} f(\mathbf{x_i})g(\theta)e^{\theta^{T} u(\mathbf{x_i})}\Big) h(\eta,\nu)g(\theta)^{\eta}e^{\theta^T\nu}d\theta\\
&amp;=\frac{h(\eta,\nu)\Big(\prod\limits_{i=1}^{N} f(\mathbf{x_i})\Big)}{h(\eta+N,\sum\limits_{i=1}^{N}u(\mathbf{x})+\nu)}\\
p(\mathbf{x} \vert D_c, \eta, \nu)&amp;= \int f(\mathbf{x})g(\theta)e^{\theta^T u(\mathbf{x})}\Big(\prod\limits_{i=1}^{N} f(\mathbf{x_i})g(\theta)e^{\theta^{T} u(\mathbf{x_i})}\Big)  \times \\
&amp; h(\eta,\nu)g(\theta)^{\eta}e^{\theta^T\nu}d\theta\\
&amp;=\frac{h(\eta,\nu)f(x)\Big(\prod\limits_{i=1}^{N} f(\mathbf{x_i})\Big)}{p(D_c \vert \eta,\nu) h{(\eta+N+1,\nu+\sum_{i=1}^{N}u(\mathbf{x_i})+u(\mathbf{x})})}\\
score(\mathbf{x})&amp;=\frac{ h(\eta+1,\nu+u(\mathbf{x}))  h(\eta+N,\nu+\sum\limits_{i=1}^{N}u(\mathbf{x}_i))}{h(\eta,\nu) h{(\eta+N+1,\nu+\sum_{i=1}^{N}u(\mathbf{x_i})+u(\mathbf{x})})}
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Ghahramani, Zoubin, and Katherine A. Heller. “Bayesian sets.” NIPS. Vol. 2. 2005. &lt;a href=&quot;https://papers.nips.cc/paper/2817-bayesian-sets.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Verma, Saurabh, and Estevam R. Hruschka Jr. “Coupled bayesian sets algorithm for semi-supervised learning and information extraction.” Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer Berlin Heidelberg, 2012. &lt;a href=&quot;http://rtw.ml.cmu.edu/papers/ecml12-verma.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 01 Jun 2016 19:01:00 -0500</pubDate>
        <link>http://localhost:4000/tutorials/2016/06/01/bayesians-sets.html</link>
        <guid isPermaLink="true">http://localhost:4000/tutorials/2016/06/01/bayesians-sets.html</guid>
        
        
        <category>Tutorials</category>
        
      </item>
    
      <item>
        <title> Blog: Why Regularized Auto-Encoders learn Sparse Representation?</title>
        <description>&lt;p&gt;This paper has some great insights to offer in design of autoencoders. As title suggests, it addresses “whether regularized helps in learning sparse representation of the data theoretically?”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Here is the setup:&lt;/strong&gt; we have an input &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x} \in \mathbf{R}^{d}&lt;/script&gt; which is mapped to latent space &lt;script type=&quot;math/tex&quot;&gt;\mathbf{h=s(Wx+b)}&lt;/script&gt; via autoencoder where &lt;script type=&quot;math/tex&quot;&gt;\mathbf{s}&lt;/script&gt; is encoder activation function, &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W} \in \mathbf{R}^{m\times n}&lt;/script&gt; is the weight matrix, and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{b} \in \mathbf{R^m}&lt;/script&gt; is the encoder bias and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{h} \in \mathbf{R^m}&lt;/script&gt; is hidden representation or outputs of hidden units.&lt;/p&gt;

&lt;p&gt;For analysis, paper assumes that decoder is linear i.e. &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W^{T}h}&lt;/script&gt; decodes back the encoded hidden representation and loss is squared loss function. Therefore, for learning &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W,b}&lt;/script&gt; parameters of autoencoder; objective function is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{split}
J_{AE} &amp;=\mathbb{E_x}[(\mathbf{x-W^Th})^2] \\
&amp;= \mathbb{E_x}[(\mathbf{x-W^Ts(Wx+b)})^2]\\
\end{split}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Now we are interested in sparsity of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{h}&lt;/script&gt;, hidden representation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Besides above, paper makes two more assumptions.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Assumption 1&lt;/strong&gt;: Data is drawn from a distribution &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x} \sim  X&lt;/script&gt; for which &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E_x}[\mathbf{x}]=0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E_x}[\mathbf{xx^T]=I}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;\mathbf{I}&lt;/script&gt; is identity matrix.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Basically, it assumes that data is whitened which is reasonable for some cases. There is one more assumption from analysis point of view which is needed for the derivation of theorems, we will see that later.&lt;/p&gt;

&lt;p&gt;Finally, for a give data sample &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt;, each hidden unit &lt;script type=&quot;math/tex&quot;&gt;h_j&lt;/script&gt; gets activated if pre-activation unit &lt;script type=&quot;math/tex&quot;&gt;a_j&lt;/script&gt; is greater than &lt;script type=&quot;math/tex&quot;&gt;\delta&lt;/script&gt; threshold:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_j=W_j\mathbf{x}+b_j&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_j=s(a_j)&lt;/script&gt;

&lt;p&gt;So, the sparsity of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{h}&lt;/script&gt; depends upon the value of pre-activation units (whose value further depend upon the input data samples). If the expected value of pre-activation unit is less than threshold over data distribution then the corresponding hidden unit expected output is  zero.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The idea is to somehow show that regularization encourages the expected value of pre-activation unit &lt;script type=&quot;math/tex&quot;&gt;a_j, \forall j&lt;/script&gt;  to reduce on average in autoencoders. This indirectly induces sparsity in &lt;script type=&quot;math/tex&quot;&gt;\mathbf{h}&lt;/script&gt;, hidden representation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;This should hint us that:&lt;/strong&gt; monotonically increasing activation functions would be preferable because we want to decrease hidden unit value as the value of pre-activation unit decreases. On the top of that, if we consider monotonically increasing activation functions with negative saturation at &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;, i.e. &lt;script type=&quot;math/tex&quot;&gt;\lim_{a\to-\infty} s(a)=0&lt;/script&gt;  then we can make sure that lower average pre-activation value implies higher sparsity. For example, consider &lt;script type=&quot;math/tex&quot;&gt;tanh&lt;/script&gt; function whose range is &lt;script type=&quot;math/tex&quot;&gt;(-1,1)&lt;/script&gt;. If we reduce &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; below zero, hidden unit output will move away from &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Let’s focus on analysis now. Assume we want to compute &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W,b}&lt;/script&gt; using gradient descent. Then at &lt;script type=&quot;math/tex&quot;&gt;t^{th}&lt;/script&gt; iteration of updating, we will have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_j^{t}=W_j^{t}\mathbf{x}+b_j^{t}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;a_j^{t}, W_j^{t}, b_j^{t}&lt;/script&gt; are values in &lt;script type=&quot;math/tex&quot;&gt;t^{th}&lt;/script&gt; iteration.&lt;/p&gt;

&lt;p&gt;Let regularized objective function of autoencoder is expressed as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J_{RAE}=J_{AE}+\lambda R(\mathbf{W,b})&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;R(\mathbf{W,b})&lt;/script&gt; is the regularization term and &lt;script type=&quot;math/tex&quot;&gt;\lambda&gt;0&lt;/script&gt;. Simplifying the main theorem of the paper over here:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;: For each iteration: &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\mathbb{E_x}[a_j^{t+1}]&lt;\mathbb{E_x}[a_j^{t}] %]]&gt;&lt;/script&gt;  holds for &lt;script type=&quot;math/tex&quot;&gt;\forall j&lt;/script&gt; with probability &lt;script type=&quot;math/tex&quot;&gt;(1-\frac{\sigma^2}{\epsilon^2})&lt;/script&gt;, if following conditions are met: &lt;script type=&quot;math/tex&quot;&gt;\hspace{5em}\lambda \frac{\partial R}{\partial b_j} &gt; 2\epsilon \sqrt{n}  \|W_j\|&lt;/script&gt; and encoding activation function, &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;, first derivative is bounded in &lt;script type=&quot;math/tex&quot;&gt;[0,1]&lt;/script&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The above theorem shows that  updating &lt;script type=&quot;math/tex&quot;&gt;\mathbf{W,b}&lt;/script&gt; along the negative gradient of &lt;script type=&quot;math/tex&quot;&gt;J_{RAE}&lt;/script&gt; results in &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\mathbb{E_x}[a_j^{t+1}]&lt;\mathbb{E_x}[a_j^{t}] %]]&gt;&lt;/script&gt; with high probability if particular regularization condition is met (plus activation function condition). This means certain regularization terms explicitly encourages sparsity in hidden representation.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Corollary 1&lt;/strong&gt;: Theorem 1 holds for two special cases: &lt;br /&gt;
1)  activation function &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; is non-decreasing and regularization term has form &lt;script type=&quot;math/tex&quot;&gt;R=\sum\limits_{j=1}^{m}f(\mathbb{E_x}[h_j])&lt;/script&gt; for some monotonically increasing function &lt;script type=&quot;math/tex&quot;&gt;f.&lt;/script&gt;&lt;br /&gt;
2)  activation function &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; is convex plus non-decreasing and regularization term has form &lt;script type=&quot;math/tex&quot;&gt;R=\mathbb{E_x}[\sum\limits_{j=1}^{m}(\frac{\partial h_j}{\partial a_j})^q \|W_j\|^{p}_2]&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; is natural and &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; whole number.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;We are ready to make some remarks about in practice activation functions!&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;ReLu&lt;/strong&gt;: It satisfies both corollaries. Hence can be used with any regularized of first form but not the second form (because second derivative does not exist). The advantage of ReLU is
that it enforces hard zeros in the learned representations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Softplus&lt;/strong&gt;: It satisfies both corollaries and encourages sparsity for both suggested regularization form. But does not produce hard zeros.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Sigmoid&lt;/strong&gt;: Satisfies only first corollary.  Hence sigmoid is not guaranteed to lead to sparsity when used with second regularizations form.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Maxout and Tanh&lt;/strong&gt;: Do not satisfies negative saturation property at &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; and hence may or may not lead to sparsity.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Final remarks about about in practice regularized autoencoders!&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;De-noising Autoencoder (DAE) has regularization of second form.&lt;/li&gt;
  &lt;li&gt;Contractive Auto-Encoder (CAE) including higher order has  regularization of second form.&lt;/li&gt;
  &lt;li&gt;Marginalized De-noising Auto-Encoder (mDAE) has  regularization of second form.&lt;/li&gt;
  &lt;li&gt;Sparse Auto-Encoder (SAE) has regularization of first form.&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
  &lt;p&gt;Thus, all above autoencoders encourages sparsity theoretically, if chosen with appropriate activation function.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Arpit, D., Zhou, Y., Ngo, H., &amp;amp; Govindaraju, V. (2015). Why Regularized Auto-Encoders learn Sparse Representation? ICML2016. &lt;a href=&quot;http://arxiv.org/pdf/1505.05561.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Tue, 31 May 2016 17:37:00 -0500</pubDate>
        <link>http://localhost:4000/paperlist/2016/05/31/why-autoencoder-sparse.html</link>
        <guid isPermaLink="true">http://localhost:4000/paperlist/2016/05/31/why-autoencoder-sparse.html</guid>
        
        
        <category>PaperList</category>
        
      </item>
    
      <item>
        <title>Blog: Learning Convolutional Networks for Graphs</title>
        <description>&lt;p&gt;For past couple of months, I have been wondering about how convolutional networks can be applied to  graphs. As we know, convolutional networks have became the state of the art in image classification and also in natural language processing. It is easy to see how convolutional networks  exploits the locality and translation invariance properties in an image. People have now also realized on how to exploit those properties in NLP using convolutional networks efficiently. It is time to look at other, more general, domains such as  &lt;em&gt;graphs&lt;/em&gt; where notion of locality or receptive field needs to be defined. At this point, we can start thinking about graph neighborhood concepts as it is going to play a major role in connecting with receptive fields of convolutional networks.&lt;/p&gt;

&lt;p&gt;There are two problem formulation exist in literature:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Given a set of graphs &lt;script type=&quot;math/tex&quot;&gt;S_G=\{G_1,G_2,...,G_n\}&lt;/script&gt; and labels &lt;script type=&quot;math/tex&quot;&gt;Y=\{y_1,y_2,...,y_n\}\in \mathbf{R}&lt;/script&gt;, learn a function &lt;script type=&quot;math/tex&quot;&gt;f:G\rightarrow\mathbf{R}&lt;/script&gt; such that it maps graphs to appropriate labels using convolutional networks.&lt;/li&gt;
    &lt;li&gt;Generalization of convolutional network to a single graph &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; where each input example is a vertex.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;The first problem is somewhat becomes usual machine learning problem, if we know how to construct/extract features from a graph and then we can simply train any machine learning algorithm. Second problem, is different in the sense that each node itself is a data point in a given graph, so it is not apparent what could be a potential feature vector here. Personally, I am more interested in second the one.&lt;/p&gt;

&lt;p&gt;Without diving into previous works (there are many), I’ll layout the findings of:  &lt;em&gt;Learning Convolutional Neural Networks for Graphs&lt;/em&gt; ICML2016 paper which deal with the first problem formulation. The idea is to bring an order among nodes in a graph. So authors introduce the notion of graph labeling which decide the rank of nodes. Following steps are involved in their overall approach:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;First compute a graph labeling to define rank on the nodes. For example, if &lt;script type=&quot;math/tex&quot;&gt;l(u) &gt; l(v)&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;r(u)&gt;r(v)&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;l(u)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;r(u)&lt;/script&gt; is the label &amp;amp; rank of node &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; respectively. This labeling can be done based on degree of node, page-rank etc.&lt;/li&gt;
    &lt;li&gt;Next sort the vertices/nodes of the input graph with respect to a given graph labeling. Then the resulting node sequence, say &lt;script type=&quot;math/tex&quot;&gt;NS&lt;/script&gt; which is 1-dimensional, is traversed using a given stride &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;.&lt;/li&gt;
    &lt;li&gt;For each visited node (through stride &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;) in &lt;script type=&quot;math/tex&quot;&gt;NS&lt;/script&gt;, construct a receptive field, until exactly &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; receptive fields have been created. (Wait. What is receptive field here ?)&lt;/li&gt;
    &lt;li&gt;Receptive field is constructed based on the neighborhood of a node. This neighborhood can be defined based on adjacent nodes or shortest distance between nodes. Given   a node &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; and the size of the receptive field &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, the procedure performs a breadth-first search to find exactly &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; neighbor nodes. Special care is taken when there are less  or more than &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; (in case of tie) neighbor nodes. We can call this neighborhood of a node as sub-graph neighborhood.&lt;/li&gt;
    &lt;li&gt;On each sub-graph neighborhood, final and crucial step is performed called graph normalization and canonicalization.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;To understand what graph canonicalization is, you need to understand what is graph isomorphism. Loosely speaking, if two graphs say &lt;script type=&quot;math/tex&quot;&gt;G_1&lt;/script&gt; &amp;amp; &lt;script type=&quot;math/tex&quot;&gt;G_2&lt;/script&gt; have different vertex labels and edge labels but graph &lt;script type=&quot;math/tex&quot;&gt;G_1&lt;/script&gt; can be re-labeled such that adjacency matrix becomes equal to graph &lt;script type=&quot;math/tex&quot;&gt;G_2&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;G_1&lt;/script&gt; is isomorphic to &lt;script type=&quot;math/tex&quot;&gt;G_2&lt;/script&gt;. In other words they are the same (in structure), its just their labellings are different. So, how do we know if two graphs are isomorphic or not. If somehow, we can represent each graph in some form, say a string of binary numbers, then we can just check if the binary representation of two graphs is same or not. This will tell us whether graphs are isomorphic or not. Of-course, this representation needs to hold some properties for such type of checking. This representation through which we can determine isomorphism is called canonical representation of a graph or &lt;script type=&quot;math/tex&quot;&gt;canon(G)&lt;/script&gt;.  Basically, in canonical representation you can compare the two graphs just as you can compare the two images of cat. Canonicalization is a NP-hard problem, so necessary optimization is required.
Finally:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Each canonical representation of sub-graph neighborhood  becomes the receptive field  and   combined with the normal convolutional networks. The vertex (and edge) attributes becomes the channel (like three RGB channels in an image).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So, magic lies in canonicalization of a graph for which author resorts to something called &lt;em&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Nauty&lt;/code&gt;&lt;/em&gt; software, which performs canonicalization. I don’t know the details of the canonicalization algorithm but it does take degree of a node into account.&lt;/p&gt;

&lt;p&gt;This is the high-level approach authors adopt and experimentally it outperform the classification accuracy obtained by graph kernel algorithms for many datasets (eg. protein structure and chemical compounds related).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Niepert, Mathias, Mohamed Ahmed, and Konstantin Kutzkov. “Learning Convolutional Neural Networks for Graphs.” ICML 2016. &lt;a href=&quot;https://arxiv.org/pdf/1605.05273v2.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Henaff, Mikael, Joan Bruna, and Yann LeCun. “Deep convolutional networks on graph-structured data.” arXiv preprint arXiv:1506.05163 (2015).&lt;a href=&quot;http://arxiv.org/pdf/1506.05163v1.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Bruna, Joan, et al. “Spectral networks and locally connected networks on graphs.” arXiv preprint arXiv:1312.6203 (2013). &lt;a href=&quot;https://arxiv.org/pdf/1312.6203.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Coates, Adam, and Andrew Y. Ng. “Selecting receptive fields in deep networks.” Advances in Neural Information Processing Systems. 2011. &lt;a href=&quot;http://robotics.stanford.edu/~ang/papers/nips11-SelectingReceptiveFields.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Yanardag, Pinar, and S. V. N. Vishwanathan. “Deep graph kernels.” Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2015.&lt;a href=&quot;http://web.ics.purdue.edu/~ypinar/kdd/deep_graph_kernels.pdf&quot;&gt;[PDF]&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 29 May 2016 17:37:00 -0500</pubDate>
        <link>http://localhost:4000/paperlist/2016/05/29/convgraph.html</link>
        <guid isPermaLink="true">http://localhost:4000/paperlist/2016/05/29/convgraph.html</guid>
        
        
        <category>PaperList</category>
        
      </item>
    
      <item>
        <title>Reading List: ICML Papers 2016 </title>
        <description>&lt;p&gt;To make a habit of reading more papers, I have decided to write comments, may be some quick or sometime detail, on  the papers which I find interesting and related to my research area. Hopefully, this will come handy in solving my own research problems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Starting with ICML 2016 Papers:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Revisiting Semi-Supervised Learning with Graph Embeddings ICML 2016&lt;/li&gt;
  &lt;li&gt;Why Regularized Auto-Encoders learn  Sparse Representation? ICML 2016&lt;/li&gt;
  &lt;li&gt;On the Consistency of Feature Selection With Lasso for Non-linear Targets. ICML 2016&lt;/li&gt;
  &lt;li&gt;Additive Approximations in High Dimensional Nonparametric Regression via the SALSA. ICML 2016&lt;/li&gt;
  &lt;li&gt;The Variational Nystrom method for large-scale spectral problems. ICML 2016&lt;/li&gt;
  &lt;li&gt;A Kernelized Stein Discrepancy for Goodness-of-fit Tests and Model Evaluation. ICML 2016 (Possibly a new way to measure difference in probability distribution other than most common – KL divergence )&lt;/li&gt;
  &lt;li&gt;Low-Rank Matrix Approximation with Stability. ICML 2016&lt;/li&gt;
  &lt;li&gt;Unsupervised Deep Embedding for Clustering Analysis. ICML 2016&lt;/li&gt;
  &lt;li&gt;Online Low-Rank Subspace Clustering by Explicit Basis Modeling. ICML 2016&lt;/li&gt;
  &lt;li&gt;Community Recovery in Graphs with Locality. ICML 2016&lt;/li&gt;
  &lt;li&gt;Analysis of Deep Neural Networks with Extended Data Jacobian Matrix. ICML 2016&lt;/li&gt;
  &lt;li&gt;Stochastic Variance Reduced Optimization for Nonconvex Sparse Learning. ICML 2016&lt;/li&gt;
  &lt;li&gt;Compressive Spectral Clustering ICML. 2016&lt;/li&gt;
  &lt;li&gt;Variance-Reduced and Projection-Free Stochastic Optimization. ICML 2016&lt;/li&gt;
  &lt;li&gt;Learning Convolutional Neural Networks for Graphs. ICML 2016&lt;/li&gt;
  &lt;li&gt;Discrete Deep Feature Extraction: A Theory and New Architectures.  ICML 2016&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 28 May 2016 17:37:00 -0500</pubDate>
        <link>http://localhost:4000/paperlist/2016/05/28/ReadingListICML16.html</link>
        <guid isPermaLink="true">http://localhost:4000/paperlist/2016/05/28/ReadingListICML16.html</guid>
        
        
        <category>PaperList</category>
        
      </item>
    
  </channel>
</rss>
